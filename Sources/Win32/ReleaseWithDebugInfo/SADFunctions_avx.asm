; Listing generated by Microsoft (R) Optimizing Compiler Version 19.00.24215.1 

	TITLE	c:\github\mvtools\sources\sadfunctions_avx.cpp
	.686P
	.XMM
	include listing.inc
	.model	flat

INCLUDELIB OLDNAMES

?piecewise_construct@std@@3Upiecewise_construct_t@1@B	ORG $+1 ; std::piecewise_construct
PUBLIC	?get_sad_avx_C_function@@YAP6AIPBEH0H@ZHHHW4arch_t@@@Z ; get_sad_avx_C_function
PUBLIC	??$Sad16_sse2_avx@$03$01G@@YAIPBEH0H@Z		; Sad16_sse2_avx<4,2,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$03$03G@@YAIPBEH0H@Z		; Sad16_sse2_avx<4,4,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$03$07G@@YAIPBEH0H@Z		; Sad16_sse2_avx<4,8,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$07$00G@@YAIPBEH0H@Z		; Sad16_sse2_avx<8,1,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$07$01G@@YAIPBEH0H@Z		; Sad16_sse2_avx<8,2,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$07$03G@@YAIPBEH0H@Z		; Sad16_sse2_avx<8,4,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$07$07G@@YAIPBEH0H@Z		; Sad16_sse2_avx<8,8,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$07$0BA@G@@YAIPBEH0H@Z	; Sad16_sse2_avx<8,16,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$0BA@$00G@@YAIPBEH0H@Z	; Sad16_sse2_avx<16,1,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$0BA@$01G@@YAIPBEH0H@Z	; Sad16_sse2_avx<16,2,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$0BA@$03G@@YAIPBEH0H@Z	; Sad16_sse2_avx<16,4,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$0BA@$07G@@YAIPBEH0H@Z	; Sad16_sse2_avx<16,8,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$0BA@$0BA@G@@YAIPBEH0H@Z	; Sad16_sse2_avx<16,16,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$0BA@$0CA@G@@YAIPBEH0H@Z	; Sad16_sse2_avx<16,32,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$0CA@$07G@@YAIPBEH0H@Z	; Sad16_sse2_avx<32,8,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$0CA@$0BA@G@@YAIPBEH0H@Z	; Sad16_sse2_avx<32,16,unsigned short>
PUBLIC	??$Sad16_sse2_avx@$0CA@$0CA@G@@YAIPBEH0H@Z	; Sad16_sse2_avx<32,32,unsigned short>
;	COMDAT xdata$x
xdata$x	SEGMENT
__ehfuncinfo$?get_sad_avx_C_function@@YAP6AIPBEH0H@ZHHHW4arch_t@@@Z DD 019930522H
	DD	01H
	DD	FLAT:__unwindtable$?get_sad_avx_C_function@@YAP6AIPBEH0H@ZHHHW4arch_t@@@Z
	DD	2 DUP(00H)
	DD	2 DUP(00H)
	DD	00H
	DD	01H
__unwindtable$?get_sad_avx_C_function@@YAP6AIPBEH0H@ZHHHW4arch_t@@@Z DD 0ffffffffH
	DD	FLAT:__unwindfunclet$?get_sad_avx_C_function@@YAP6AIPBEH0H@ZHHHW4arch_t@@@Z$0
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$0CA@$0CA@G@@YAIPBEH0H@Z
_TEXT	SEGMENT
tv567 = -12						; size = 4
tv564 = -8						; size = 4
tv563 = -4						; size = 4
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$0CA@$0CA@G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<32,32,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H
	sub	esp, 16					; 00000010H
	mov	eax, DWORD PTR _nSrcPitch$[ebp]
	add	eax, eax

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2

	mov	DWORD PTR tv567[esp+16], 16		; 00000010H
	mov	DWORD PTR tv564[esp+16], eax
	mov	eax, DWORD PTR _nRefPitch$[ebp]
	add	eax, eax
	push	esi
	vpxor	xmm4, xmm4, xmm4
	push	edi
	mov	edi, DWORD PTR _pSrc$[ebp]
	mov	DWORD PTR tv563[esp+24], eax
	mov	eax, DWORD PTR _pRef$[ebp]

; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32

	vmovdqa	xmm5, xmm4
$LL4@Sad16_sse2:

; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x
; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));
; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x
; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)

	mov	edx, edi
	mov	esi, eax
	mov	edi, DWORD PTR _nSrcPitch$[ebp]
	sub	edx, eax
	mov	eax, 4
$LL7@Sad16_sse2:

; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));

	lea	ecx, DWORD PTR [edx+esi]
	lea	esi, DWORD PTR [esi+16]
	vmovdqu	xmm2, XMMWORD PTR [ecx]

; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));

	vmovdqu	xmm0, XMMWORD PTR [esi-16]

; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));

	vmovdqu	xmm2, XMMWORD PTR [ecx+edi]

; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));

	mov	ecx, DWORD PTR _nRefPitch$[ebp]
	vpor	xmm3, xmm1, xmm0
	vmovdqu	xmm0, XMMWORD PTR [esi+ecx-16]

; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))

	vpor	xmm2, xmm1, xmm0
	vpunpcklwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm0, xmm5
	vpunpckhwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));

	vpunpcklwd xmm0, xmm2, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));

	vpunpckhwd xmm0, xmm2, xmm4
	vpaddd	xmm5, xmm1, xmm0
	sub	eax, 1
	jne	SHORT $LL7@Sad16_sse2

; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;

	mov	edi, DWORD PTR _pSrc$[ebp]

; 226  :         pRef += nRefPitch * 2;

	mov	eax, DWORD PTR _pRef$[ebp]
	add	edi, DWORD PTR tv564[esp+24]
	add	eax, DWORD PTR tv563[esp+24]
	sub	DWORD PTR tv567[esp+24], 1
	mov	DWORD PTR _pSrc$[ebp], edi
	mov	DWORD PTR _pRef$[ebp], eax
	jne	$LL4@Sad16_sse2

; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0

	vpunpckldq xmm0, xmm5, xmm4

; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm5, xmm4

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm4

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);
; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	pop	edi
	vmovd	eax, xmm0
	pop	esi
	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$0CA@$0CA@G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<32,32,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$0CA@$0BA@G@@YAIPBEH0H@Z
_TEXT	SEGMENT
tv567 = -12						; size = 4
tv564 = -8						; size = 4
tv563 = -4						; size = 4
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$0CA@$0BA@G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<32,16,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H
	sub	esp, 16					; 00000010H
	mov	eax, DWORD PTR _nSrcPitch$[ebp]
	add	eax, eax

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2

	mov	DWORD PTR tv567[esp+16], 8
	mov	DWORD PTR tv564[esp+16], eax
	mov	eax, DWORD PTR _nRefPitch$[ebp]
	add	eax, eax
	push	esi
	vpxor	xmm4, xmm4, xmm4
	push	edi
	mov	edi, DWORD PTR _pSrc$[ebp]
	mov	DWORD PTR tv563[esp+24], eax
	mov	eax, DWORD PTR _pRef$[ebp]

; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32

	vmovdqa	xmm5, xmm4
$LL4@Sad16_sse2:

; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x
; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));
; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x
; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)

	mov	edx, edi
	mov	esi, eax
	mov	edi, DWORD PTR _nSrcPitch$[ebp]
	sub	edx, eax
	mov	eax, 4
$LL7@Sad16_sse2:

; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));

	lea	ecx, DWORD PTR [edx+esi]
	lea	esi, DWORD PTR [esi+16]
	vmovdqu	xmm2, XMMWORD PTR [ecx]

; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));

	vmovdqu	xmm0, XMMWORD PTR [esi-16]

; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));

	vmovdqu	xmm2, XMMWORD PTR [ecx+edi]

; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));

	mov	ecx, DWORD PTR _nRefPitch$[ebp]
	vpor	xmm3, xmm1, xmm0
	vmovdqu	xmm0, XMMWORD PTR [esi+ecx-16]

; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))

	vpor	xmm2, xmm1, xmm0
	vpunpcklwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm0, xmm5
	vpunpckhwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));

	vpunpcklwd xmm0, xmm2, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));

	vpunpckhwd xmm0, xmm2, xmm4
	vpaddd	xmm5, xmm1, xmm0
	sub	eax, 1
	jne	SHORT $LL7@Sad16_sse2

; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;

	mov	edi, DWORD PTR _pSrc$[ebp]

; 226  :         pRef += nRefPitch * 2;

	mov	eax, DWORD PTR _pRef$[ebp]
	add	edi, DWORD PTR tv564[esp+24]
	add	eax, DWORD PTR tv563[esp+24]
	sub	DWORD PTR tv567[esp+24], 1
	mov	DWORD PTR _pSrc$[ebp], edi
	mov	DWORD PTR _pRef$[ebp], eax
	jne	$LL4@Sad16_sse2

; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0

	vpunpckldq xmm0, xmm5, xmm4

; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm5, xmm4

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm4

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);
; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	pop	edi
	vmovd	eax, xmm0
	pop	esi
	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$0CA@$0BA@G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<32,16,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$0CA@$07G@@YAIPBEH0H@Z
_TEXT	SEGMENT
tv567 = -12						; size = 4
tv564 = -8						; size = 4
tv563 = -4						; size = 4
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$0CA@$07G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<32,8,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H
	sub	esp, 16					; 00000010H
	mov	eax, DWORD PTR _nSrcPitch$[ebp]
	add	eax, eax

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2

	mov	DWORD PTR tv567[esp+16], 4
	mov	DWORD PTR tv564[esp+16], eax
	mov	eax, DWORD PTR _nRefPitch$[ebp]
	add	eax, eax
	push	esi
	vpxor	xmm4, xmm4, xmm4
	push	edi
	mov	edi, DWORD PTR _pSrc$[ebp]
	mov	DWORD PTR tv563[esp+24], eax
	mov	eax, DWORD PTR _pRef$[ebp]

; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32

	vmovdqa	xmm5, xmm4
$LL4@Sad16_sse2:

; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x
; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));
; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x
; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)

	mov	edx, edi
	mov	esi, eax
	mov	edi, DWORD PTR _nSrcPitch$[ebp]
	sub	edx, eax
	mov	eax, 4
$LL7@Sad16_sse2:

; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));

	lea	ecx, DWORD PTR [edx+esi]
	lea	esi, DWORD PTR [esi+16]
	vmovdqu	xmm2, XMMWORD PTR [ecx]

; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));

	vmovdqu	xmm0, XMMWORD PTR [esi-16]

; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));

	vmovdqu	xmm2, XMMWORD PTR [ecx+edi]

; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));

	mov	ecx, DWORD PTR _nRefPitch$[ebp]
	vpor	xmm3, xmm1, xmm0
	vmovdqu	xmm0, XMMWORD PTR [esi+ecx-16]

; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))

	vpor	xmm2, xmm1, xmm0
	vpunpcklwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm0, xmm5
	vpunpckhwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));

	vpunpcklwd xmm0, xmm2, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));

	vpunpckhwd xmm0, xmm2, xmm4
	vpaddd	xmm5, xmm1, xmm0
	sub	eax, 1
	jne	SHORT $LL7@Sad16_sse2

; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;

	mov	edi, DWORD PTR _pSrc$[ebp]

; 226  :         pRef += nRefPitch * 2;

	mov	eax, DWORD PTR _pRef$[ebp]
	add	edi, DWORD PTR tv564[esp+24]
	add	eax, DWORD PTR tv563[esp+24]
	sub	DWORD PTR tv567[esp+24], 1
	mov	DWORD PTR _pSrc$[ebp], edi
	mov	DWORD PTR _pRef$[ebp], eax
	jne	$LL4@Sad16_sse2

; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0

	vpunpckldq xmm0, xmm5, xmm4

; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm5, xmm4

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm4

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);
; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	pop	edi
	vmovd	eax, xmm0
	pop	esi
	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$0CA@$07G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<32,8,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$0BA@$0CA@G@@YAIPBEH0H@Z
_TEXT	SEGMENT
tv567 = -12						; size = 4
tv564 = -8						; size = 4
tv563 = -4						; size = 4
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$0BA@$0CA@G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<16,32,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H
	sub	esp, 16					; 00000010H
	mov	eax, DWORD PTR _nSrcPitch$[ebp]
	add	eax, eax

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2

	mov	DWORD PTR tv567[esp+16], 16		; 00000010H
	mov	DWORD PTR tv564[esp+16], eax
	mov	eax, DWORD PTR _nRefPitch$[ebp]
	add	eax, eax
	push	esi
	vpxor	xmm4, xmm4, xmm4
	push	edi
	mov	edi, DWORD PTR _pSrc$[ebp]
	mov	DWORD PTR tv563[esp+24], eax
	mov	eax, DWORD PTR _pRef$[ebp]

; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32

	vmovdqa	xmm5, xmm4
$LL4@Sad16_sse2:

; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x
; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));
; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x
; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)

	mov	edx, edi
	mov	esi, eax
	mov	edi, DWORD PTR _nSrcPitch$[ebp]
	sub	edx, eax
	mov	eax, 2
$LL7@Sad16_sse2:

; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));

	lea	ecx, DWORD PTR [edx+esi]
	lea	esi, DWORD PTR [esi+16]
	vmovdqu	xmm2, XMMWORD PTR [ecx]

; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));

	vmovdqu	xmm0, XMMWORD PTR [esi-16]

; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));

	vmovdqu	xmm2, XMMWORD PTR [ecx+edi]

; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));

	mov	ecx, DWORD PTR _nRefPitch$[ebp]
	vpor	xmm3, xmm1, xmm0
	vmovdqu	xmm0, XMMWORD PTR [esi+ecx-16]

; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))

	vpor	xmm2, xmm1, xmm0
	vpunpcklwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm0, xmm5
	vpunpckhwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));

	vpunpcklwd xmm0, xmm2, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));

	vpunpckhwd xmm0, xmm2, xmm4
	vpaddd	xmm5, xmm1, xmm0
	sub	eax, 1
	jne	SHORT $LL7@Sad16_sse2

; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;

	mov	edi, DWORD PTR _pSrc$[ebp]

; 226  :         pRef += nRefPitch * 2;

	mov	eax, DWORD PTR _pRef$[ebp]
	add	edi, DWORD PTR tv564[esp+24]
	add	eax, DWORD PTR tv563[esp+24]
	sub	DWORD PTR tv567[esp+24], 1
	mov	DWORD PTR _pSrc$[ebp], edi
	mov	DWORD PTR _pRef$[ebp], eax
	jne	$LL4@Sad16_sse2

; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0

	vpunpckldq xmm0, xmm5, xmm4

; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm5, xmm4

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm4

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);
; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	pop	edi
	vmovd	eax, xmm0
	pop	esi
	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$0BA@$0CA@G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<16,32,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$0BA@$0BA@G@@YAIPBEH0H@Z
_TEXT	SEGMENT
tv567 = -12						; size = 4
tv564 = -8						; size = 4
tv563 = -4						; size = 4
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$0BA@$0BA@G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<16,16,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H
	sub	esp, 16					; 00000010H
	mov	eax, DWORD PTR _nSrcPitch$[ebp]
	add	eax, eax

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2

	mov	DWORD PTR tv567[esp+16], 8
	mov	DWORD PTR tv564[esp+16], eax
	mov	eax, DWORD PTR _nRefPitch$[ebp]
	add	eax, eax
	push	esi
	vpxor	xmm4, xmm4, xmm4
	push	edi
	mov	edi, DWORD PTR _pSrc$[ebp]
	mov	DWORD PTR tv563[esp+24], eax
	mov	eax, DWORD PTR _pRef$[ebp]

; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32

	vmovdqa	xmm5, xmm4
$LL4@Sad16_sse2:

; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x
; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));
; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x
; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)

	mov	edx, edi
	mov	esi, eax
	mov	edi, DWORD PTR _nSrcPitch$[ebp]
	sub	edx, eax
	mov	eax, 2
$LL7@Sad16_sse2:

; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));

	lea	ecx, DWORD PTR [edx+esi]
	lea	esi, DWORD PTR [esi+16]
	vmovdqu	xmm2, XMMWORD PTR [ecx]

; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));

	vmovdqu	xmm0, XMMWORD PTR [esi-16]

; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));

	vmovdqu	xmm2, XMMWORD PTR [ecx+edi]

; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));

	mov	ecx, DWORD PTR _nRefPitch$[ebp]
	vpor	xmm3, xmm1, xmm0
	vmovdqu	xmm0, XMMWORD PTR [esi+ecx-16]

; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))

	vpor	xmm2, xmm1, xmm0
	vpunpcklwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm0, xmm5
	vpunpckhwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));

	vpunpcklwd xmm0, xmm2, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));

	vpunpckhwd xmm0, xmm2, xmm4
	vpaddd	xmm5, xmm1, xmm0
	sub	eax, 1
	jne	SHORT $LL7@Sad16_sse2

; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;

	mov	edi, DWORD PTR _pSrc$[ebp]

; 226  :         pRef += nRefPitch * 2;

	mov	eax, DWORD PTR _pRef$[ebp]
	add	edi, DWORD PTR tv564[esp+24]
	add	eax, DWORD PTR tv563[esp+24]
	sub	DWORD PTR tv567[esp+24], 1
	mov	DWORD PTR _pSrc$[ebp], edi
	mov	DWORD PTR _pRef$[ebp], eax
	jne	$LL4@Sad16_sse2

; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0

	vpunpckldq xmm0, xmm5, xmm4

; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm5, xmm4

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm4

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);
; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	pop	edi
	vmovd	eax, xmm0
	pop	esi
	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$0BA@$0BA@G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<16,16,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$0BA@$07G@@YAIPBEH0H@Z
_TEXT	SEGMENT
tv567 = -12						; size = 4
tv564 = -8						; size = 4
tv563 = -4						; size = 4
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$0BA@$07G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<16,8,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H
	sub	esp, 16					; 00000010H
	mov	eax, DWORD PTR _nSrcPitch$[ebp]
	add	eax, eax

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2

	mov	DWORD PTR tv567[esp+16], 4
	mov	DWORD PTR tv564[esp+16], eax
	mov	eax, DWORD PTR _nRefPitch$[ebp]
	add	eax, eax
	push	esi
	vpxor	xmm4, xmm4, xmm4
	push	edi
	mov	edi, DWORD PTR _pSrc$[ebp]
	mov	DWORD PTR tv563[esp+24], eax
	mov	eax, DWORD PTR _pRef$[ebp]

; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32

	vmovdqa	xmm5, xmm4
$LL4@Sad16_sse2:

; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x
; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));
; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x
; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)

	mov	edx, edi
	mov	esi, eax
	mov	edi, DWORD PTR _nSrcPitch$[ebp]
	sub	edx, eax
	mov	eax, 2
$LL7@Sad16_sse2:

; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));

	lea	ecx, DWORD PTR [edx+esi]
	lea	esi, DWORD PTR [esi+16]
	vmovdqu	xmm2, XMMWORD PTR [ecx]

; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));

	vmovdqu	xmm0, XMMWORD PTR [esi-16]

; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));

	vmovdqu	xmm2, XMMWORD PTR [ecx+edi]

; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));

	mov	ecx, DWORD PTR _nRefPitch$[ebp]
	vpor	xmm3, xmm1, xmm0
	vmovdqu	xmm0, XMMWORD PTR [esi+ecx-16]

; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))

	vpor	xmm2, xmm1, xmm0
	vpunpcklwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm0, xmm5
	vpunpckhwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));

	vpunpcklwd xmm0, xmm2, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));

	vpunpckhwd xmm0, xmm2, xmm4
	vpaddd	xmm5, xmm1, xmm0
	sub	eax, 1
	jne	SHORT $LL7@Sad16_sse2

; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;

	mov	edi, DWORD PTR _pSrc$[ebp]

; 226  :         pRef += nRefPitch * 2;

	mov	eax, DWORD PTR _pRef$[ebp]
	add	edi, DWORD PTR tv564[esp+24]
	add	eax, DWORD PTR tv563[esp+24]
	sub	DWORD PTR tv567[esp+24], 1
	mov	DWORD PTR _pSrc$[ebp], edi
	mov	DWORD PTR _pRef$[ebp], eax
	jne	$LL4@Sad16_sse2

; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0

	vpunpckldq xmm0, xmm5, xmm4

; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm5, xmm4

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm4

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);
; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	pop	edi
	vmovd	eax, xmm0
	pop	esi
	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$0BA@$07G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<16,8,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$0BA@$03G@@YAIPBEH0H@Z
_TEXT	SEGMENT
tv567 = -12						; size = 4
tv564 = -8						; size = 4
tv563 = -4						; size = 4
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$0BA@$03G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<16,4,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H
	sub	esp, 16					; 00000010H
	mov	eax, DWORD PTR _nSrcPitch$[ebp]
	add	eax, eax

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2

	mov	DWORD PTR tv567[esp+16], 2
	mov	DWORD PTR tv564[esp+16], eax
	mov	eax, DWORD PTR _nRefPitch$[ebp]
	add	eax, eax
	push	esi
	vpxor	xmm4, xmm4, xmm4
	push	edi
	mov	edi, DWORD PTR _pSrc$[ebp]
	mov	DWORD PTR tv563[esp+24], eax
	mov	eax, DWORD PTR _pRef$[ebp]

; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32

	vmovdqa	xmm5, xmm4
$LL4@Sad16_sse2:

; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x
; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));
; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x
; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)

	mov	edx, edi
	mov	esi, eax
	mov	edi, DWORD PTR _nSrcPitch$[ebp]
	sub	edx, eax
	mov	eax, 2
$LL7@Sad16_sse2:

; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));

	lea	ecx, DWORD PTR [edx+esi]
	lea	esi, DWORD PTR [esi+16]
	vmovdqu	xmm2, XMMWORD PTR [ecx]

; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));

	vmovdqu	xmm0, XMMWORD PTR [esi-16]

; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));

	vmovdqu	xmm2, XMMWORD PTR [ecx+edi]

; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));

	mov	ecx, DWORD PTR _nRefPitch$[ebp]
	vpor	xmm3, xmm1, xmm0
	vmovdqu	xmm0, XMMWORD PTR [esi+ecx-16]

; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))

	vpor	xmm2, xmm1, xmm0
	vpunpcklwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm0, xmm5
	vpunpckhwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));

	vpunpcklwd xmm0, xmm2, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));

	vpunpckhwd xmm0, xmm2, xmm4
	vpaddd	xmm5, xmm1, xmm0
	sub	eax, 1
	jne	SHORT $LL7@Sad16_sse2

; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;

	mov	edi, DWORD PTR _pSrc$[ebp]

; 226  :         pRef += nRefPitch * 2;

	mov	eax, DWORD PTR _pRef$[ebp]
	add	edi, DWORD PTR tv564[esp+24]
	add	eax, DWORD PTR tv563[esp+24]
	sub	DWORD PTR tv567[esp+24], 1
	mov	DWORD PTR _pSrc$[ebp], edi
	mov	DWORD PTR _pRef$[ebp], eax
	jne	$LL4@Sad16_sse2

; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0

	vpunpckldq xmm0, xmm5, xmm4

; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm5, xmm4

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm4

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);
; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	pop	edi
	vmovd	eax, xmm0
	pop	esi
	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$0BA@$03G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<16,4,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$0BA@$01G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$0BA@$01G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<16,2,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2
; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32
; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x
; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));
; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x
; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)

	mov	edx, DWORD PTR _pSrc$[ebp]
	mov	eax, DWORD PTR _pRef$[ebp]
	sub	edx, eax
	push	esi
	vpxor	xmm4, xmm4, xmm4
	push	edi
	vmovdqa	xmm5, xmm4
	mov	esi, 2
	npad	3
$LL7@Sad16_sse2:

; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));
; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));
; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));

	mov	edi, DWORD PTR _nSrcPitch$[ebp]
	lea	ecx, DWORD PTR [edx+eax]
	lea	eax, DWORD PTR [eax+16]
	vmovdqu	xmm2, XMMWORD PTR [ecx]
	vmovdqu	xmm0, XMMWORD PTR [eax-16]
	vpsubusw xmm1, xmm2, xmm0
	vpsubusw xmm0, xmm0, xmm2
	vmovdqu	xmm2, XMMWORD PTR [ecx+edi]

; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));

	mov	edi, DWORD PTR _nRefPitch$[ebp]
	vpor	xmm3, xmm1, xmm0
	vmovdqu	xmm0, XMMWORD PTR [eax+edi-16]

; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))

	vpor	xmm2, xmm1, xmm0
	vpunpcklwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm0, xmm5
	vpunpckhwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));

	vpunpcklwd xmm0, xmm2, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));

	vpunpckhwd xmm0, xmm2, xmm4
	vpaddd	xmm5, xmm1, xmm0
	sub	esi, 1
	jne	SHORT $LL7@Sad16_sse2

; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;
; 226  :         pRef += nRefPitch * 2;
; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0

	vpunpckldq xmm0, xmm5, xmm4

; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm5, xmm4

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm4

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);
; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	pop	edi
	vmovd	eax, xmm0
	pop	esi
	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$0BA@$01G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<16,2,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$0BA@$00G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$0BA@$00G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<16,1,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2
; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32
; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x
; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));
; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x
; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)

	mov	ecx, DWORD PTR _pSrc$[ebp]
	mov	edx, 2
	mov	eax, DWORD PTR _pRef$[ebp]
	sub	ecx, eax
	vpxor	xmm3, xmm3, xmm3
	vmovdqa	xmm4, xmm3
	npad	5
$LL7@Sad16_sse2:
	lea	eax, DWORD PTR [eax+16]

; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));

	vmovdqu	xmm2, XMMWORD PTR [ecx+eax-16]

; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));

	vmovdqu	xmm0, XMMWORD PTR [eax-16]

; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))

	vpor	xmm2, xmm1, xmm0

; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));

	vpunpcklwd xmm0, xmm2, xmm3
	vpaddd	xmm1, xmm0, xmm4

; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));

	vpunpckhwd xmm0, xmm2, xmm3
	vpaddd	xmm4, xmm1, xmm0
	sub	edx, 1
	jne	SHORT $LL7@Sad16_sse2

; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));
; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));
; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;
; 226  :         pRef += nRefPitch * 2;
; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0

	vpunpckldq xmm0, xmm4, xmm3

; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm4, xmm3

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm3

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);

	vmovd	eax, xmm0

; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$0BA@$00G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<16,1,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$07$0BA@G@@YAIPBEH0H@Z
_TEXT	SEGMENT
tv490 = -8						; size = 4
tv489 = -4						; size = 4
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$07$0BA@G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<8,16,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H
	sub	esp, 8
	mov	edx, DWORD PTR _nSrcPitch$[ebp]

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2

	mov	ecx, DWORD PTR _pSrc$[ebp]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[ebp]
	lea	eax, DWORD PTR [edx+edx]
	mov	DWORD PTR tv490[esp+12], eax
	vpxor	xmm4, xmm4, xmm4
	lea	eax, DWORD PTR [esi+esi]
	push	edi
	mov	DWORD PTR tv489[esp+16], eax
	mov	edi, 8
	mov	eax, DWORD PTR _pRef$[ebp]

; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32

	vmovdqa	xmm5, xmm4
$LL4@Sad16_sse2:

; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x

	vmovdqu	xmm2, XMMWORD PTR [ecx]

; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));

	vmovdqu	xmm0, XMMWORD PTR [eax]

; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x

	vmovdqu	xmm2, XMMWORD PTR [ecx+edx]

; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));
; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));
; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));
; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));
; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;

	add	ecx, DWORD PTR tv490[esp+16]
	vpor	xmm3, xmm1, xmm0
	vmovdqu	xmm0, XMMWORD PTR [eax+esi]

; 226  :         pRef += nRefPitch * 2;

	add	eax, DWORD PTR tv489[esp+16]
	vpsubusw xmm1, xmm2, xmm0
	vpsubusw xmm0, xmm0, xmm2
	vpor	xmm2, xmm1, xmm0
	vpunpcklwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm0, xmm5
	vpunpckhwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm1, xmm0
	vpunpcklwd xmm0, xmm2, xmm4
	vpaddd	xmm1, xmm1, xmm0
	vpunpckhwd xmm0, xmm2, xmm4
	vpaddd	xmm5, xmm1, xmm0
	sub	edi, 1
	jne	SHORT $LL4@Sad16_sse2

; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0

	vpunpckldq xmm0, xmm5, xmm4

; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm5, xmm4

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm4

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);
; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	pop	edi
	vmovd	eax, xmm0
	pop	esi
	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$07$0BA@G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<8,16,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$07$07G@@YAIPBEH0H@Z
_TEXT	SEGMENT
tv490 = -8						; size = 4
tv489 = -4						; size = 4
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$07$07G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<8,8,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H
	sub	esp, 8
	mov	edx, DWORD PTR _nSrcPitch$[ebp]

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2

	mov	ecx, DWORD PTR _pSrc$[ebp]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[ebp]
	lea	eax, DWORD PTR [edx+edx]
	mov	DWORD PTR tv490[esp+12], eax
	vpxor	xmm4, xmm4, xmm4
	lea	eax, DWORD PTR [esi+esi]
	push	edi
	mov	DWORD PTR tv489[esp+16], eax
	mov	edi, 4
	mov	eax, DWORD PTR _pRef$[ebp]

; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32

	vmovdqa	xmm5, xmm4
$LL4@Sad16_sse2:

; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x

	vmovdqu	xmm2, XMMWORD PTR [ecx]

; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));

	vmovdqu	xmm0, XMMWORD PTR [eax]

; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x

	vmovdqu	xmm2, XMMWORD PTR [ecx+edx]

; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));
; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));
; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));
; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));
; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;

	add	ecx, DWORD PTR tv490[esp+16]
	vpor	xmm3, xmm1, xmm0
	vmovdqu	xmm0, XMMWORD PTR [eax+esi]

; 226  :         pRef += nRefPitch * 2;

	add	eax, DWORD PTR tv489[esp+16]
	vpsubusw xmm1, xmm2, xmm0
	vpsubusw xmm0, xmm0, xmm2
	vpor	xmm2, xmm1, xmm0
	vpunpcklwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm0, xmm5
	vpunpckhwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm1, xmm0
	vpunpcklwd xmm0, xmm2, xmm4
	vpaddd	xmm1, xmm1, xmm0
	vpunpckhwd xmm0, xmm2, xmm4
	vpaddd	xmm5, xmm1, xmm0
	sub	edi, 1
	jne	SHORT $LL4@Sad16_sse2

; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0

	vpunpckldq xmm0, xmm5, xmm4

; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm5, xmm4

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm4

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);
; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	pop	edi
	vmovd	eax, xmm0
	pop	esi
	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$07$07G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<8,8,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$07$03G@@YAIPBEH0H@Z
_TEXT	SEGMENT
tv490 = -8						; size = 4
tv489 = -4						; size = 4
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$07$03G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<8,4,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H
	sub	esp, 8
	mov	edx, DWORD PTR _nSrcPitch$[ebp]

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2

	mov	ecx, DWORD PTR _pSrc$[ebp]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[ebp]
	lea	eax, DWORD PTR [edx+edx]
	mov	DWORD PTR tv490[esp+12], eax
	vpxor	xmm4, xmm4, xmm4
	lea	eax, DWORD PTR [esi+esi]
	push	edi
	mov	DWORD PTR tv489[esp+16], eax
	mov	edi, 2
	mov	eax, DWORD PTR _pRef$[ebp]

; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32

	vmovdqa	xmm5, xmm4
$LL4@Sad16_sse2:

; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x

	vmovdqu	xmm2, XMMWORD PTR [ecx]

; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));

	vmovdqu	xmm0, XMMWORD PTR [eax]

; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x

	vmovdqu	xmm2, XMMWORD PTR [ecx+edx]

; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));
; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));
; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));
; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));
; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;

	add	ecx, DWORD PTR tv490[esp+16]
	vpor	xmm3, xmm1, xmm0
	vmovdqu	xmm0, XMMWORD PTR [eax+esi]

; 226  :         pRef += nRefPitch * 2;

	add	eax, DWORD PTR tv489[esp+16]
	vpsubusw xmm1, xmm2, xmm0
	vpsubusw xmm0, xmm0, xmm2
	vpor	xmm2, xmm1, xmm0
	vpunpcklwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm0, xmm5
	vpunpckhwd xmm0, xmm3, xmm4
	vpaddd	xmm1, xmm1, xmm0
	vpunpcklwd xmm0, xmm2, xmm4
	vpaddd	xmm1, xmm1, xmm0
	vpunpckhwd xmm0, xmm2, xmm4
	vpaddd	xmm5, xmm1, xmm0
	sub	edi, 1
	jne	SHORT $LL4@Sad16_sse2

; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0

	vpunpckldq xmm0, xmm5, xmm4

; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm5, xmm4

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm4

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);
; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	pop	edi
	vmovd	eax, xmm0
	pop	esi
	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$07$03G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<8,4,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$07$01G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$07$01G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<8,2,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2
; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32
; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x

	mov	edx, DWORD PTR _pSrc$[ebp]

; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));

	mov	ecx, DWORD PTR _pRef$[ebp]

; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x

	mov	eax, DWORD PTR _nSrcPitch$[ebp]
	vpxor	xmm4, xmm4, xmm4
	vmovdqu	xmm2, XMMWORD PTR [edx]
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vpsubusw xmm1, xmm2, xmm0
	vpsubusw xmm0, xmm0, xmm2
	vmovdqu	xmm2, XMMWORD PTR [edx+eax]

; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));

	mov	eax, DWORD PTR _nRefPitch$[ebp]
	vpor	xmm3, xmm1, xmm0
	vmovdqu	xmm0, XMMWORD PTR [ecx+eax]

; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))

	vpor	xmm2, xmm1, xmm0
	vpunpckhwd xmm0, xmm3, xmm4
	vpunpcklwd xmm1, xmm3, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));

	vpunpcklwd xmm0, xmm2, xmm4
	vpaddd	xmm1, xmm1, xmm0

; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));

	vpunpckhwd xmm0, xmm2, xmm4
	vpaddd	xmm0, xmm1, xmm0

; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));
; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));
; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));
; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));
; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;
; 226  :         pRef += nRefPitch * 2;
; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0
; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm0, xmm4
	vpunpckldq xmm0, xmm0, xmm4

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm4

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);

	vmovd	eax, xmm0

; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$07$01G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<8,2,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$07$00G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$07$00G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<8,1,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2
; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32
; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x

	mov	eax, DWORD PTR _pSrc$[ebp]
	vpxor	xmm3, xmm3, xmm3
	vmovdqu	xmm2, XMMWORD PTR [eax]

; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));

	mov	eax, DWORD PTR _pRef$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]

; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))

	vpor	xmm2, xmm1, xmm0

; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));

	vpunpcklwd xmm1, xmm2, xmm3

; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));

	vpunpckhwd xmm0, xmm2, xmm3
	vpaddd	xmm0, xmm1, xmm0

; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x
; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));
; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));
; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));
; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));
; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;
; 226  :         pRef += nRefPitch * 2;
; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0
; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm0, xmm3
	vpunpckldq xmm0, xmm0, xmm3

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm3

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);

	vmovd	eax, xmm0

; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$07$00G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<8,1,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$03$07G@@YAIPBEH0H@Z
_TEXT	SEGMENT
tv495 = -8						; size = 4
tv494 = -4						; size = 4
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$03$07G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<4,8,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H
	sub	esp, 8
	mov	edx, DWORD PTR _nSrcPitch$[ebp]

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2

	mov	ecx, DWORD PTR _pSrc$[ebp]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[ebp]
	lea	eax, DWORD PTR [edx+edx]
	mov	DWORD PTR tv495[esp+12], eax
	vpxor	xmm3, xmm3, xmm3
	lea	eax, DWORD PTR [esi+esi]
	push	edi
	mov	DWORD PTR tv494[esp+16], eax
	mov	edi, 4
	mov	eax, DWORD PTR _pRef$[ebp]

; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32

	vmovdqa	xmm4, xmm3
$LL4@Sad16_sse2:

; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));

	vmovq	xmm1, QWORD PTR [ecx]
	vmovq	xmm0, QWORD PTR [ecx+edx]

; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x
; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));
; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x
; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));
; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));
; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));
; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));
; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;

	add	ecx, DWORD PTR tv495[esp+16]
	vpunpcklwd xmm2, xmm1, xmm0
	vmovq	xmm1, QWORD PTR [eax]
	vmovq	xmm0, QWORD PTR [eax+esi]

; 226  :         pRef += nRefPitch * 2;

	add	eax, DWORD PTR tv494[esp+16]
	vpunpcklwd xmm0, xmm1, xmm0
	vpsubusw xmm1, xmm2, xmm0
	vpsubusw xmm0, xmm0, xmm2
	vpor	xmm2, xmm1, xmm0
	vpunpcklwd xmm0, xmm2, xmm3
	vpaddd	xmm1, xmm0, xmm4
	vpunpckhwd xmm0, xmm2, xmm3
	vpaddd	xmm4, xmm1, xmm0
	sub	edi, 1
	jne	SHORT $LL4@Sad16_sse2

; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0

	vpunpckldq xmm0, xmm4, xmm3

; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm4, xmm3

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm3

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);
; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	pop	edi
	vmovd	eax, xmm0
	pop	esi
	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$03$07G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<4,8,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$03$03G@@YAIPBEH0H@Z
_TEXT	SEGMENT
tv495 = -8						; size = 4
tv494 = -4						; size = 4
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$03$03G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<4,4,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H
	sub	esp, 8
	mov	edx, DWORD PTR _nSrcPitch$[ebp]

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2

	mov	ecx, DWORD PTR _pSrc$[ebp]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[ebp]
	lea	eax, DWORD PTR [edx+edx]
	mov	DWORD PTR tv495[esp+12], eax
	vpxor	xmm3, xmm3, xmm3
	lea	eax, DWORD PTR [esi+esi]
	push	edi
	mov	DWORD PTR tv494[esp+16], eax
	mov	edi, 2
	mov	eax, DWORD PTR _pRef$[ebp]

; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32

	vmovdqa	xmm4, xmm3
$LL4@Sad16_sse2:

; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));

	vmovq	xmm1, QWORD PTR [ecx]
	vmovq	xmm0, QWORD PTR [ecx+edx]

; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x
; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));
; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x
; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));
; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));
; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));
; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));
; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;

	add	ecx, DWORD PTR tv495[esp+16]
	vpunpcklwd xmm2, xmm1, xmm0
	vmovq	xmm1, QWORD PTR [eax]
	vmovq	xmm0, QWORD PTR [eax+esi]

; 226  :         pRef += nRefPitch * 2;

	add	eax, DWORD PTR tv494[esp+16]
	vpunpcklwd xmm0, xmm1, xmm0
	vpsubusw xmm1, xmm2, xmm0
	vpsubusw xmm0, xmm0, xmm2
	vpor	xmm2, xmm1, xmm0
	vpunpcklwd xmm0, xmm2, xmm3
	vpaddd	xmm1, xmm0, xmm4
	vpunpckhwd xmm0, xmm2, xmm3
	vpaddd	xmm4, xmm1, xmm0
	sub	edi, 1
	jne	SHORT $LL4@Sad16_sse2

; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0

	vpunpckldq xmm0, xmm4, xmm3

; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm4, xmm3

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm3

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);
; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	pop	edi
	vmovd	eax, xmm0
	pop	esi
	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$03$03G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<4,4,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad16_sse2_avx@$03$01G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad16_sse2_avx@$03$01G@@YAIPBEH0H@Z PROC		; Sad16_sse2_avx<4,2,unsigned short>, COMDAT

; 88   : {

	push	ebp
	mov	ebp, esp
	and	esp, -8					; fffffff8H

; 89   : #if 0
; 90   :   // check result against C
; 91   :   unsigned int result2 = Sad_AVX_C<nBlkWidth, nBlkHeight, pixel_t>(pSrc, nSrcPitch, pRef, nRefPitch);
; 92   : #endif
; 93   :   _mm256_zeroupper(); // diff from main sse2
; 94   :   __m128i zero = _mm_setzero_si128();
; 95   :   __m128i sum = _mm_setzero_si128(); // 2x or 4x int is probably enough for 32x32
; 96   : 
; 97   :   const bool two_8byte_rows = (sizeof(pixel_t) == 2 && nBlkWidth <= 4) || (sizeof(pixel_t) == 1 && nBlkWidth <= 8);
; 98   :   const bool one_cycle = (sizeof(pixel_t) * nBlkWidth) == 16;
; 99   :   const bool unroll_by2 = !two_8byte_rows && nBlkHeight>=2; // unroll by 4: slower
; 100  : 
; 101  :   bool unaligned = true;
; 102  :   if (!two_8byte_rows) {
; 103  :     // test. not faster. Checking overhead?
; 104  :     unaligned = true; // ((reinterpret_cast <ptrdiff_t> (pSrc) & 0x0F) || ((reinterpret_cast <ptrdiff_t>(pRef)) & 0x0F) || ((nRefPitch | nSrcPitch) & 0x0F));
; 105  :   }
; 106  : 
; 107  :   if (unaligned) {
; 108  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows || unroll_by2) ? 2 : 1)
; 109  :     {
; 110  :       if (two_8byte_rows) { // no x cycle
; 111  :         __m128i src1, src2;
; 112  :         // (8 bytes or 4 words) * 2 rows
; 113  : #if 0
; 114  :         src1 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)), 8));
; 115  :         src2 = _mm_or_si128(_mm_loadl_epi64((__m128i *) (pRef)), _mm_slli_si128(_mm_loadl_epi64((__m128i *) (pRef + nRefPitch)), 8));
; 116  : #else
; 117  :         // 16.12.01 unpack
; 118  :         if (sizeof(pixel_t) == 1) {
; 119  :           src1 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));
; 120  :           src2 = _mm_unpacklo_epi8(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));
; 121  :         }
; 122  :         else if (sizeof(pixel_t) == 2) {
; 123  :           src1 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pSrc)), _mm_loadl_epi64((__m128i *) (pSrc + nSrcPitch)));

	mov	ecx, DWORD PTR _pSrc$[ebp]
	mov	eax, DWORD PTR _nSrcPitch$[ebp]
	vpxor	xmm3, xmm3, xmm3
	vmovq	xmm1, QWORD PTR [ecx]
	vmovq	xmm0, QWORD PTR [ecx+eax]

; 124  :           src2 = _mm_unpacklo_epi16(_mm_loadl_epi64((__m128i *) (pRef)), _mm_loadl_epi64((__m128i *) (pRef + nRefPitch)));

	mov	ecx, DWORD PTR _pRef$[ebp]
	mov	eax, DWORD PTR _nRefPitch$[ebp]
	vpunpcklwd xmm2, xmm1, xmm0
	vmovq	xmm1, QWORD PTR [ecx]
	vmovq	xmm0, QWORD PTR [ecx+eax]
	vpunpcklwd xmm0, xmm1, xmm0

; 125  :         }
; 126  : #endif
; 127  :         if (sizeof(pixel_t) == 1) {
; 128  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 129  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 130  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 131  :         }
; 132  :         else {
; 133  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation

	vpsubusw xmm1, xmm2, xmm0

; 134  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);

	vpsubusw xmm0, xmm0, xmm2

; 135  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))

	vpor	xmm2, xmm1, xmm0

; 136  :           // 8 x uint16 absolute differences
; 137  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));

	vpunpcklwd xmm1, xmm2, xmm3

; 138  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));

	vpunpckhwd xmm0, xmm2, xmm3
	vpaddd	xmm0, xmm1, xmm0

; 139  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 140  :         }
; 141  :       }
; 142  :       else if (one_cycle)
; 143  :       {
; 144  :         __m128i src1, src2;
; 145  :         src1 = _mm_loadu_si128((__m128i *) (pSrc)); // no x
; 146  :         src2 = _mm_loadu_si128((__m128i *) (pRef));
; 147  :         if (sizeof(pixel_t) == 1) {
; 148  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 149  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 150  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 151  :         }
; 152  :         else {
; 153  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 154  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 155  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 156  :                                                                 // 8 x uint16 absolute differences
; 157  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 158  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 159  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 160  :         }
; 161  :         if (unroll_by2) {
; 162  :           // unroll#2
; 163  :           src1 = _mm_loadu_si128((__m128i *) (pSrc+nSrcPitch)); // no x
; 164  :           src2 = _mm_loadu_si128((__m128i *) (pRef+nRefPitch));
; 165  :           if (sizeof(pixel_t) == 1) {
; 166  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 167  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 168  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 169  :           }
; 170  :           else {
; 171  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 172  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 173  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 174  :                                                                   // 8 x uint16 absolute differences
; 175  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 176  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 177  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 178  :           }
; 179  :         }
; 180  :       }
; 181  :       else {
; 182  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 183  :         {
; 184  :           __m128i src1, src2;
; 185  :           src1 = _mm_loadu_si128((__m128i *) (pSrc + x));
; 186  :           src2 = _mm_loadu_si128((__m128i *) (pRef + x));
; 187  :           if (sizeof(pixel_t) == 1) {
; 188  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 189  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 190  :             // result in two 32 bit areas at the upper and lower 64 bytes
; 191  :           }
; 192  :           else {
; 193  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 194  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 195  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 196  :             // 8 x uint16 absolute differences
; 197  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 198  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 199  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 200  :           }
; 201  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 202  :           if (unroll_by2)
; 203  :           {
; 204  :             // unroll#2
; 205  :             src1 = _mm_loadu_si128((__m128i *) (pSrc + nSrcPitch + x));
; 206  :             src2 = _mm_loadu_si128((__m128i *) (pRef + nRefPitch + x));
; 207  :             if (sizeof(pixel_t) == 1) {
; 208  :               // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 209  :               sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 210  :                                                                   // result in two 32 bit areas at the upper and lower 64 bytes
; 211  :             }
; 212  :             else {
; 213  :               __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 214  :               __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 215  :               __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 216  :                                                                     // 8 x uint16 absolute differences
; 217  :               sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 218  :               sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 219  :               // sum1_32, sum2_32, sum3_32, sum4_32
; 220  :             }
; 221  :           }
; 222  :         }
; 223  :       }
; 224  :       if (two_8byte_rows || unroll_by2) {
; 225  :         pSrc += nSrcPitch * 2;
; 226  :         pRef += nRefPitch * 2;
; 227  :       }
; 228  :       else {
; 229  :         pSrc += nSrcPitch;
; 230  :         pRef += nRefPitch;
; 231  :       }
; 232  :     }
; 233  :   }
; 234  : #if 0
; 235  :   else { // aligned, experimental. With the in-function alignment check overhead it is slower
; 236  :     for (int y = 0; y < nBlkHeight; y += (two_8byte_rows ? 2 : 1))
; 237  :     {
; 238  :       if (one_cycle)
; 239  :       {
; 240  :         __m128i src1, src2;
; 241  :         src1 = _mm_load_si128((__m128i *) (pSrc)); // no x
; 242  :         src2 = _mm_load_si128((__m128i *) (pRef));
; 243  :         if (sizeof(pixel_t) == 1) {
; 244  :           // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 245  :           sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 246  :                                                               // result in two 32 bit areas at the upper and lower 64 bytes
; 247  :         }
; 248  :         else {
; 249  :           __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 250  :           __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 251  :           __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 252  :                                                                 // 8 x uint16 absolute differences
; 253  :           sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 254  :           sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 255  :           // sum1_32, sum2_32, sum3_32, sum4_32
; 256  :         }
; 257  :       }
; 258  :       else {
; 259  :         for (int x = 0; x < nBlkWidth * sizeof(pixel_t); x += 16)
; 260  :         {
; 261  :           __m128i src1, src2;
; 262  :           src1 = _mm_load_si128((__m128i *) (pSrc + x));
; 263  :           src2 = _mm_load_si128((__m128i *) (pRef + x));
; 264  :           if (sizeof(pixel_t) == 1) {
; 265  :             // this is uint_16 specific, but will test on uint8_t against external .asm SAD functions)
; 266  :             sum = _mm_add_epi32(sum, _mm_sad_epu8(src1, src2)); // yihhaaa, existing SIMD   sum1_32, 0, sum2_32, 0
; 267  :                                                                 // result in two 32 bit areas at the upper and lower 64 bytes
; 268  :           }
; 269  :           else {
; 270  :             __m128i greater_t = _mm_subs_epu16(src1, src2); // unsigned sub with saturation
; 271  :             __m128i smaller_t = _mm_subs_epu16(src2, src1);
; 272  :             __m128i absdiff = _mm_or_si128(greater_t, smaller_t); //abs(s1-s2)  == (satsub(s1,s2) | satsub(s2,s1))
; 273  :                                                                   // 8 x uint16 absolute differences
; 274  :             sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(absdiff, zero));
; 275  :             sum = _mm_add_epi32(sum, _mm_unpackhi_epi16(absdiff, zero));
; 276  :             // sum1_32, sum2_32, sum3_32, sum4_32
; 277  :           }
; 278  :           // sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);
; 279  :         }
; 280  :       }
; 281  :       if (two_8byte_rows) {
; 282  :         pSrc += nSrcPitch * 2;
; 283  :         pRef += nRefPitch * 2;
; 284  :       }
; 285  :       else {
; 286  :         pSrc += nSrcPitch;
; 287  :         pRef += nRefPitch;
; 288  :       }
; 289  :     }
; 290  : 
; 291  :   }
; 292  : #endif
; 293  :   /*
; 294  :   [Low64, Hi64]
; 295  :   _mm_unpacklo_epi64(_mm_setzero_si128(), x)  [0, x0]
; 296  :   _mm_unpackhi_epi64(_mm_setzero_si128(), x)  [0, x1]
; 297  :   _mm_move_epi64(x)                           [x0, 0]
; 298  :   _mm_unpackhi_epi64(x, _mm_setzero_si128())  [x1, 0]
; 299  :   */
; 300  : #if 1
; 301  :   if(sizeof(pixel_t) == 2) {
; 302  :     // at 16 bits: we have 4 integers for sum: a0 a1 a2 a3
; 303  :     __m128i a0_a1 = _mm_unpacklo_epi32(sum, zero); // a0 0 a1 0
; 304  :     __m128i a2_a3 = _mm_unpackhi_epi32(sum, zero); // a2 0 a3 0

	vpunpckhdq xmm1, xmm0, xmm3
	vpunpckldq xmm0, xmm0, xmm3

; 305  :     sum = _mm_add_epi32( a0_a1, a2_a3 ); // a0+a2, 0, a1+a3, 0

	vpaddd	xmm1, xmm1, xmm0

; 306  :     // hadd: shower
; 307  :   }
; 308  :   // sum here: two 32 bit partial result: sum1 0 sum2 0
; 309  :   __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 

	vpunpckhqdq xmm0, xmm1, xmm3

; 310  :   sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3

	vpaddd	xmm0, xmm0, xmm1

; 311  : #else
; 312  :   // this hadd path is slower on Ivy bridge
; 313  :   if (sizeof(pixel_t) == 2) {
; 314  :     sum = _mm_hadd_epi32(sum, zero); // a0_a = a0 + a1, a1_a = a2 + a3
; 315  :     sum = _mm_hadd_epi32(sum, zero); // a0 = a0_a + a1_a ( = a0+a1+a2+a3)
; 316  :   }
; 317  :   else { // uint8_t
; 318  :     // sum here: two 32 bit partial result: sum1 0 sum2 0
; 319  :     __m128i sum_hi = _mm_unpackhi_epi64(sum, zero); // a1 + a3. 2 dwords right 
; 320  :     sum = _mm_add_epi32(sum, sum_hi);  // a0 + a2 + a1 + a3
; 321  : 
; 322  :   }
; 323  : #endif
; 324  : 
; 325  :   unsigned int result = _mm_cvtsi128_si32(sum);

	vmovd	eax, xmm0

; 326  : 
; 327  : #if 0
; 328  :   // check result against C
; 329  :   if (result != result2) {
; 330  :     result = result2;
; 331  :   }
; 332  : #endif
; 333  :   _mm256_zeroupper(); // diff from main sse2
; 334  : 
; 335  :   return result;
; 336  : }  // end of SSE2 with AVX commandset Sad16

	mov	esp, ebp
	pop	ebp
	ret	0
??$Sad16_sse2_avx@$03$01G@@YAIPBEH0H@Z ENDP		; Sad16_sse2_avx<4,2,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\xtree
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\xtree
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\tuple
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\tuple
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\tuple
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\xtree
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\xtree
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\xmemory0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\xmemory0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ?get_sad_avx_C_function@@YAP6AIPBEH0H@ZHHHW4arch_t@@@Z
_TEXT	SEGMENT
$T2 = -44						; size = 16
$T3 = -44						; size = 16
$T4 = -44						; size = 16
$T5 = -44						; size = 16
$T6 = -44						; size = 16
$T7 = -44						; size = 16
$T8 = -44						; size = 16
$T9 = -44						; size = 16
$T10 = -44						; size = 16
$T11 = -44						; size = 16
$T12 = -44						; size = 16
$T13 = -44						; size = 16
$T14 = -44						; size = 16
$T15 = -44						; size = 16
$T16 = -44						; size = 16
$T17 = -44						; size = 16
$T18 = -44						; size = 16
$T19 = -44						; size = 16
$T20 = -44						; size = 16
$T21 = -44						; size = 16
$T22 = -44						; size = 16
$T23 = -44						; size = 16
$T24 = -44						; size = 16
$T25 = -44						; size = 16
$T26 = -44						; size = 16
$T27 = -44						; size = 16
$T28 = -44						; size = 16
$T29 = -44						; size = 16
$T30 = -44						; size = 16
$T31 = -44						; size = 16
$T32 = -44						; size = 16
$T33 = -44						; size = 16
$T34 = -44						; size = 16
$T35 = -44						; size = 16
$T36 = -44						; size = 16
$T37 = -44						; size = 16
$T38 = -44						; size = 16
$T39 = -44						; size = 16
$T40 = -44						; size = 16
$T41 = -44						; size = 16
$T42 = -44						; size = 16
$T43 = -44						; size = 16
$T44 = -44						; size = 16
_func_sad$ = -28					; size = 8
$T45 = -20						; size = 8
$T46 = -20						; size = 8
$T47 = -20						; size = 8
$T48 = -20						; size = 8
$T49 = -20						; size = 8
$T50 = -20						; size = 8
$T51 = -20						; size = 8
$T52 = -20						; size = 8
$T53 = -20						; size = 8
$T54 = -20						; size = 8
$T55 = -20						; size = 8
$T56 = -20						; size = 8
$T57 = -20						; size = 8
$T58 = -20						; size = 8
$T59 = -20						; size = 8
$T60 = -20						; size = 8
$T61 = -20						; size = 8
$T62 = -20						; size = 8
$T63 = -20						; size = 8
$T64 = -20						; size = 8
$T65 = -20						; size = 8
$T66 = -20						; size = 8
$T67 = -20						; size = 8
$T68 = -20						; size = 8
$T69 = -20						; size = 8
$T70 = -20						; size = 8
$T71 = -20						; size = 8
$T72 = -20						; size = 8
$T73 = -20						; size = 8
$T74 = -20						; size = 8
$T75 = -20						; size = 8
$T76 = -20						; size = 8
$T77 = -20						; size = 8
$T78 = -20						; size = 8
$T79 = -20						; size = 8
$T80 = -20						; size = 8
$T81 = -20						; size = 8
$T82 = -20						; size = 8
$T83 = -20						; size = 8
$T84 = -20						; size = 8
$T85 = -20						; size = 8
$T86 = -20						; size = 8
$T87 = -20						; size = 8
$T88 = -16						; size = 4
__$EHRec$ = -12						; size = 12
_pixelsize$ = 8						; size = 4
_arch$dead$ = 12					; size = 4
?get_sad_avx_C_function@@YAP6AIPBEH0H@ZHHHW4arch_t@@@Z PROC ; get_sad_avx_C_function, COMDAT
; _BlockX$ = ecx
; _BlockY$ = edx

; 32   : {

	push	ebp
	mov	ebp, esp
	push	-1
	push	__ehhandler$?get_sad_avx_C_function@@YAP6AIPBEH0H@ZHHHW4arch_t@@@Z
	mov	eax, DWORD PTR fs:0
	push	eax
	mov	DWORD PTR fs:0, esp
	sub	esp, 32					; 00000020H
	push	esi
	push	edi
	mov	esi, edx
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\xtree

; 567  : 		: _Myhead(),

	mov	DWORD PTR _func_sad$[ebp], 0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 32   : {

	mov	edi, ecx
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\xtree

; 568  : 		_Mysize(0)

	mov	DWORD PTR _func_sad$[ebp+4], 0

; 721  : 		_Myhead() = _Buyheadnode();

	call	?_Buyheadnode@?$_Tree_comp_alloc@V?$_Tmap_traits@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@$0A@@std@@@std@@QAEPAU?$_Tree_node@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@PAX@2@XZ ; std::_Tree_comp_alloc<std::_Tmap_traits<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> >,0> >::_Buyheadnode
	mov	DWORD PTR _func_sad$[ebp], eax
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T44[ebp]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 34   :     std::map<std::tuple<int, int, int, arch_t>, SADFunction*> func_sad;

	mov	DWORD PTR __$EHRec$[ebp+8], 0
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000020000000200000000100000000
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	push	eax
	lea	eax, DWORD PTR $T87[ebp]
	push	eax
	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqu	XMMWORD PTR $T44[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 37   :     func_sad[make_tuple(32, 32, 1, NO_SIMD)] = Sad_AVX_C<32, 32,uint8_t>;

	mov	eax, DWORD PTR $T87[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000020000000100000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 37   :     func_sad[make_tuple(32, 32, 1, NO_SIMD)] = Sad_AVX_C<32, 32,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0CA@$0CA@E@@YAIPBEH0H@Z ; Sad_AVX_C<32,32,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T43[ebp]
	push	eax
	lea	eax, DWORD PTR $T86[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T43[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 38   :     func_sad[make_tuple(32, 16, 1, NO_SIMD)] = Sad_AVX_C<32, 16,uint8_t>;

	mov	eax, DWORD PTR $T86[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000020000000080000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 38   :     func_sad[make_tuple(32, 16, 1, NO_SIMD)] = Sad_AVX_C<32, 16,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0CA@$0BA@E@@YAIPBEH0H@Z ; Sad_AVX_C<32,16,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T42[ebp]
	push	eax
	lea	eax, DWORD PTR $T85[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T42[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 39   :     func_sad[make_tuple(32, 8 , 1, NO_SIMD)] = Sad_AVX_C<32, 8,uint8_t>;

	mov	eax, DWORD PTR $T85[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000010000000200000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 39   :     func_sad[make_tuple(32, 8 , 1, NO_SIMD)] = Sad_AVX_C<32, 8,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0CA@$07E@@YAIPBEH0H@Z ; Sad_AVX_C<32,8,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T41[ebp]
	push	eax
	lea	eax, DWORD PTR $T84[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T41[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 40   :     func_sad[make_tuple(16, 32, 1, NO_SIMD)] = Sad_AVX_C<16, 32,uint8_t>;

	mov	eax, DWORD PTR $T84[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000010000000100000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 40   :     func_sad[make_tuple(16, 32, 1, NO_SIMD)] = Sad_AVX_C<16, 32,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0BA@$0CA@E@@YAIPBEH0H@Z ; Sad_AVX_C<16,32,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T40[ebp]
	push	eax
	lea	eax, DWORD PTR $T83[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T40[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 41   :     func_sad[make_tuple(16, 16, 1, NO_SIMD)] = Sad_AVX_C<16, 16,uint8_t>;

	mov	eax, DWORD PTR $T83[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000010000000080000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 41   :     func_sad[make_tuple(16, 16, 1, NO_SIMD)] = Sad_AVX_C<16, 16,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0BA@$0BA@E@@YAIPBEH0H@Z ; Sad_AVX_C<16,16,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T39[ebp]
	push	eax
	lea	eax, DWORD PTR $T82[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T39[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 42   :     func_sad[make_tuple(16, 8 , 1, NO_SIMD)] = Sad_AVX_C<16, 8,uint8_t>;

	mov	eax, DWORD PTR $T82[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000010000000040000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 42   :     func_sad[make_tuple(16, 8 , 1, NO_SIMD)] = Sad_AVX_C<16, 8,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0BA@$07E@@YAIPBEH0H@Z ; Sad_AVX_C<16,8,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T38[ebp]
	push	eax
	lea	eax, DWORD PTR $T81[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T38[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 43   :     func_sad[make_tuple(16, 4 , 1, NO_SIMD)] = Sad_AVX_C<16, 4,uint8_t>;

	mov	eax, DWORD PTR $T81[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000010000000020000000100000000
	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0BA@$03E@@YAIPBEH0H@Z ; Sad_AVX_C<16,4,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T37[ebp]
	push	eax
	lea	eax, DWORD PTR $T80[ebp]
	push	eax
	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqu	XMMWORD PTR $T37[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 44   :     func_sad[make_tuple(16, 2 , 1, NO_SIMD)] = Sad_AVX_C<16, 2,uint8_t>;

	mov	eax, DWORD PTR $T80[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000010000000010000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 44   :     func_sad[make_tuple(16, 2 , 1, NO_SIMD)] = Sad_AVX_C<16, 2,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0BA@$01E@@YAIPBEH0H@Z ; Sad_AVX_C<16,2,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T36[ebp]
	push	eax
	lea	eax, DWORD PTR $T79[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T36[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 45   :     func_sad[make_tuple(16, 1 , 1, NO_SIMD)] = Sad_AVX_C<16, 1,uint8_t>;

	mov	eax, DWORD PTR $T79[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000008000000100000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 45   :     func_sad[make_tuple(16, 1 , 1, NO_SIMD)] = Sad_AVX_C<16, 1,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0BA@$00E@@YAIPBEH0H@Z ; Sad_AVX_C<16,1,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T35[ebp]
	push	eax
	lea	eax, DWORD PTR $T78[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T35[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 46   :     func_sad[make_tuple(8 , 16, 1, NO_SIMD)] = Sad_AVX_C<8 , 16,uint8_t>;

	mov	eax, DWORD PTR $T78[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000008000000080000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 46   :     func_sad[make_tuple(8 , 16, 1, NO_SIMD)] = Sad_AVX_C<8 , 16,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$07$0BA@E@@YAIPBEH0H@Z ; Sad_AVX_C<8,16,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T34[ebp]
	push	eax
	lea	eax, DWORD PTR $T77[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T34[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 47   :     func_sad[make_tuple(8 , 8 , 1, NO_SIMD)] = Sad_AVX_C<8 , 8,uint8_t>;

	mov	eax, DWORD PTR $T77[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000008000000040000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 47   :     func_sad[make_tuple(8 , 8 , 1, NO_SIMD)] = Sad_AVX_C<8 , 8,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$07$07E@@YAIPBEH0H@Z ; Sad_AVX_C<8,8,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T33[ebp]
	push	eax
	lea	eax, DWORD PTR $T76[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T33[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 48   :     func_sad[make_tuple(8 , 4 , 1, NO_SIMD)] = Sad_AVX_C<8 , 4,uint8_t>;

	mov	eax, DWORD PTR $T76[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000008000000020000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 48   :     func_sad[make_tuple(8 , 4 , 1, NO_SIMD)] = Sad_AVX_C<8 , 4,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$07$03E@@YAIPBEH0H@Z ; Sad_AVX_C<8,4,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T32[ebp]
	push	eax
	lea	eax, DWORD PTR $T75[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T32[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 49   :     func_sad[make_tuple(8 , 2 , 1, NO_SIMD)] = Sad_AVX_C<8 , 2,uint8_t>;

	mov	eax, DWORD PTR $T75[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000008000000010000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 49   :     func_sad[make_tuple(8 , 2 , 1, NO_SIMD)] = Sad_AVX_C<8 , 2,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$07$01E@@YAIPBEH0H@Z ; Sad_AVX_C<8,2,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T31[ebp]
	push	eax
	lea	eax, DWORD PTR $T74[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T31[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 50   :     func_sad[make_tuple(8 , 1 , 1, NO_SIMD)] = Sad_AVX_C<8 , 1,uint8_t>;

	mov	eax, DWORD PTR $T74[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000004000000080000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 50   :     func_sad[make_tuple(8 , 1 , 1, NO_SIMD)] = Sad_AVX_C<8 , 1,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$07$00E@@YAIPBEH0H@Z ; Sad_AVX_C<8,1,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T30[ebp]
	push	eax
	lea	eax, DWORD PTR $T73[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T30[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 51   :     func_sad[make_tuple(4 , 8 , 1, NO_SIMD)] = Sad_AVX_C<4 , 8,uint8_t>;

	mov	eax, DWORD PTR $T73[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000004000000040000000100000000
	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$03$07E@@YAIPBEH0H@Z ; Sad_AVX_C<4,8,unsigned char>
	vmovdqu	XMMWORD PTR $T29[ebp], xmm0
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T29[ebp]
	push	eax
	lea	eax, DWORD PTR $T72[ebp]
	push	eax
	lea	ecx, DWORD PTR _func_sad$[ebp]
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 52   :     func_sad[make_tuple(4 , 4 , 1, NO_SIMD)] = Sad_AVX_C<4 , 4,uint8_t>;

	mov	eax, DWORD PTR $T72[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000004000000020000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 52   :     func_sad[make_tuple(4 , 4 , 1, NO_SIMD)] = Sad_AVX_C<4 , 4,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$03$03E@@YAIPBEH0H@Z ; Sad_AVX_C<4,4,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T28[ebp]
	push	eax
	lea	eax, DWORD PTR $T71[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T28[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 53   :     func_sad[make_tuple(4 , 2 , 1, NO_SIMD)] = Sad_AVX_C<4 , 2,uint8_t>;

	mov	eax, DWORD PTR $T71[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000004000000010000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 53   :     func_sad[make_tuple(4 , 2 , 1, NO_SIMD)] = Sad_AVX_C<4 , 2,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$03$01E@@YAIPBEH0H@Z ; Sad_AVX_C<4,2,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T27[ebp]
	push	eax
	lea	eax, DWORD PTR $T70[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T27[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 54   :     func_sad[make_tuple(4 , 1 , 1, NO_SIMD)] = Sad_AVX_C<4 , 1,uint8_t>;

	mov	eax, DWORD PTR $T70[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000002000000040000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 54   :     func_sad[make_tuple(4 , 1 , 1, NO_SIMD)] = Sad_AVX_C<4 , 1,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$03$00E@@YAIPBEH0H@Z ; Sad_AVX_C<4,1,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T26[ebp]
	push	eax
	lea	eax, DWORD PTR $T69[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T26[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 55   :     func_sad[make_tuple(2 , 4 , 1, NO_SIMD)] = Sad_AVX_C<2 , 4,uint8_t>;

	mov	eax, DWORD PTR $T69[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000002000000020000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 55   :     func_sad[make_tuple(2 , 4 , 1, NO_SIMD)] = Sad_AVX_C<2 , 4,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$01$03E@@YAIPBEH0H@Z ; Sad_AVX_C<2,4,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T25[ebp]
	push	eax
	lea	eax, DWORD PTR $T68[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T25[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 56   :     func_sad[make_tuple(2 , 2 , 1, NO_SIMD)] = Sad_AVX_C<2 , 2,uint8_t>;

	mov	eax, DWORD PTR $T68[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000002000000010000000100000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 56   :     func_sad[make_tuple(2 , 2 , 1, NO_SIMD)] = Sad_AVX_C<2 , 2,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$01$01E@@YAIPBEH0H@Z ; Sad_AVX_C<2,2,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T24[ebp]
	push	eax
	lea	eax, DWORD PTR $T67[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T24[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 57   :     func_sad[make_tuple(2 , 1 , 1, NO_SIMD)] = Sad_AVX_C<2 , 1,uint8_t>;

	mov	eax, DWORD PTR $T67[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000020000000200000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 57   :     func_sad[make_tuple(2 , 1 , 1, NO_SIMD)] = Sad_AVX_C<2 , 1,uint8_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$01$00E@@YAIPBEH0H@Z ; Sad_AVX_C<2,1,unsigned char>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T23[ebp]
	push	eax
	lea	eax, DWORD PTR $T66[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T23[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 59   :     func_sad[make_tuple(32, 32, 2, NO_SIMD)] = Sad_AVX_C<32, 32,uint16_t>;

	mov	eax, DWORD PTR $T66[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000020000000100000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 59   :     func_sad[make_tuple(32, 32, 2, NO_SIMD)] = Sad_AVX_C<32, 32,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0CA@$0CA@G@@YAIPBEH0H@Z ; Sad_AVX_C<32,32,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T22[ebp]
	push	eax
	lea	eax, DWORD PTR $T65[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T22[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 60   :     func_sad[make_tuple(32, 16, 2, NO_SIMD)] = Sad_AVX_C<32, 16,uint16_t>;

	mov	eax, DWORD PTR $T65[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000020000000080000000200000000
	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0CA@$0BA@G@@YAIPBEH0H@Z ; Sad_AVX_C<32,16,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T21[ebp]
	vmovdqu	XMMWORD PTR $T21[ebp], xmm0
	push	eax
	lea	eax, DWORD PTR $T64[ebp]
	push	eax
	lea	ecx, DWORD PTR _func_sad$[ebp]
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 61   :     func_sad[make_tuple(32, 8 , 2, NO_SIMD)] = Sad_AVX_C<32, 8,uint16_t>;

	mov	eax, DWORD PTR $T64[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000010000000200000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 61   :     func_sad[make_tuple(32, 8 , 2, NO_SIMD)] = Sad_AVX_C<32, 8,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0CA@$07G@@YAIPBEH0H@Z ; Sad_AVX_C<32,8,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T20[ebp]
	push	eax
	lea	eax, DWORD PTR $T63[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T20[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 62   :     func_sad[make_tuple(16, 32, 2, NO_SIMD)] = Sad_AVX_C<16, 32,uint16_t>;

	mov	eax, DWORD PTR $T63[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000010000000100000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 62   :     func_sad[make_tuple(16, 32, 2, NO_SIMD)] = Sad_AVX_C<16, 32,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0BA@$0CA@G@@YAIPBEH0H@Z ; Sad_AVX_C<16,32,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T19[ebp]
	push	eax
	lea	eax, DWORD PTR $T62[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T19[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 63   :     func_sad[make_tuple(16, 16, 2, NO_SIMD)] = Sad_AVX_C<16, 16,uint16_t>;

	mov	eax, DWORD PTR $T62[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000010000000080000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 63   :     func_sad[make_tuple(16, 16, 2, NO_SIMD)] = Sad_AVX_C<16, 16,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0BA@$0BA@G@@YAIPBEH0H@Z ; Sad_AVX_C<16,16,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T18[ebp]
	push	eax
	lea	eax, DWORD PTR $T61[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T18[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 64   :     func_sad[make_tuple(16, 8 , 2, NO_SIMD)] = Sad_AVX_C<16, 8,uint16_t>;

	mov	eax, DWORD PTR $T61[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000010000000040000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 64   :     func_sad[make_tuple(16, 8 , 2, NO_SIMD)] = Sad_AVX_C<16, 8,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0BA@$07G@@YAIPBEH0H@Z ; Sad_AVX_C<16,8,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T17[ebp]
	push	eax
	lea	eax, DWORD PTR $T60[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T17[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 65   :     func_sad[make_tuple(16, 4 , 2, NO_SIMD)] = Sad_AVX_C<16, 4,uint16_t>;

	mov	eax, DWORD PTR $T60[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000010000000020000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 65   :     func_sad[make_tuple(16, 4 , 2, NO_SIMD)] = Sad_AVX_C<16, 4,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0BA@$03G@@YAIPBEH0H@Z ; Sad_AVX_C<16,4,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T16[ebp]
	push	eax
	lea	eax, DWORD PTR $T59[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T16[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 66   :     func_sad[make_tuple(16, 2 , 2, NO_SIMD)] = Sad_AVX_C<16, 2,uint16_t>;

	mov	eax, DWORD PTR $T59[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000010000000010000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 66   :     func_sad[make_tuple(16, 2 , 2, NO_SIMD)] = Sad_AVX_C<16, 2,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0BA@$01G@@YAIPBEH0H@Z ; Sad_AVX_C<16,2,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T15[ebp]
	push	eax
	lea	eax, DWORD PTR $T58[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T15[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 67   :     func_sad[make_tuple(16, 1 , 2, NO_SIMD)] = Sad_AVX_C<16, 1,uint16_t>;

	mov	eax, DWORD PTR $T58[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000008000000100000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 67   :     func_sad[make_tuple(16, 1 , 2, NO_SIMD)] = Sad_AVX_C<16, 1,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$0BA@$00G@@YAIPBEH0H@Z ; Sad_AVX_C<16,1,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T14[ebp]
	push	eax
	lea	eax, DWORD PTR $T57[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T14[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 68   :     func_sad[make_tuple(8 , 16, 2, NO_SIMD)] = Sad_AVX_C<8 , 16,uint16_t>;

	mov	eax, DWORD PTR $T57[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000008000000080000000200000000
	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$07$0BA@G@@YAIPBEH0H@Z ; Sad_AVX_C<8,16,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T13[ebp]
	vmovdqu	XMMWORD PTR $T13[ebp], xmm0
	push	eax
	lea	eax, DWORD PTR $T56[ebp]
	push	eax
	lea	ecx, DWORD PTR _func_sad$[ebp]
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 69   :     func_sad[make_tuple(8 , 8 , 2, NO_SIMD)] = Sad_AVX_C<8 , 8,uint16_t>;

	mov	eax, DWORD PTR $T56[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000008000000040000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 69   :     func_sad[make_tuple(8 , 8 , 2, NO_SIMD)] = Sad_AVX_C<8 , 8,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$07$07G@@YAIPBEH0H@Z ; Sad_AVX_C<8,8,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T12[ebp]
	push	eax
	lea	eax, DWORD PTR $T55[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T12[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 70   :     func_sad[make_tuple(8 , 4 , 2, NO_SIMD)] = Sad_AVX_C<8 , 4,uint16_t>;

	mov	eax, DWORD PTR $T55[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000008000000020000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 70   :     func_sad[make_tuple(8 , 4 , 2, NO_SIMD)] = Sad_AVX_C<8 , 4,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$07$03G@@YAIPBEH0H@Z ; Sad_AVX_C<8,4,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T11[ebp]
	push	eax
	lea	eax, DWORD PTR $T54[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T11[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 71   :     func_sad[make_tuple(8 , 2 , 2, NO_SIMD)] = Sad_AVX_C<8 , 2,uint16_t>;

	mov	eax, DWORD PTR $T54[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000008000000010000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 71   :     func_sad[make_tuple(8 , 2 , 2, NO_SIMD)] = Sad_AVX_C<8 , 2,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$07$01G@@YAIPBEH0H@Z ; Sad_AVX_C<8,2,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T10[ebp]
	push	eax
	lea	eax, DWORD PTR $T53[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T10[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 72   :     func_sad[make_tuple(8 , 1 , 2, NO_SIMD)] = Sad_AVX_C<8 , 1,uint16_t>;

	mov	eax, DWORD PTR $T53[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000004000000080000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 72   :     func_sad[make_tuple(8 , 1 , 2, NO_SIMD)] = Sad_AVX_C<8 , 1,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$07$00G@@YAIPBEH0H@Z ; Sad_AVX_C<8,1,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T9[ebp]
	push	eax
	lea	eax, DWORD PTR $T52[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T9[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 73   :     func_sad[make_tuple(4 , 8 , 2, NO_SIMD)] = Sad_AVX_C<4 , 8,uint16_t>;

	mov	eax, DWORD PTR $T52[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000004000000040000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 73   :     func_sad[make_tuple(4 , 8 , 2, NO_SIMD)] = Sad_AVX_C<4 , 8,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$03$07G@@YAIPBEH0H@Z ; Sad_AVX_C<4,8,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T8[ebp]
	push	eax
	lea	eax, DWORD PTR $T51[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T8[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 74   :     func_sad[make_tuple(4 , 4 , 2, NO_SIMD)] = Sad_AVX_C<4 , 4,uint16_t>;

	mov	eax, DWORD PTR $T51[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000004000000020000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 74   :     func_sad[make_tuple(4 , 4 , 2, NO_SIMD)] = Sad_AVX_C<4 , 4,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$03$03G@@YAIPBEH0H@Z ; Sad_AVX_C<4,4,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T7[ebp]
	push	eax
	lea	eax, DWORD PTR $T50[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T7[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 75   :     func_sad[make_tuple(4 , 2 , 2, NO_SIMD)] = Sad_AVX_C<4 , 2,uint16_t>;

	mov	eax, DWORD PTR $T50[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000004000000010000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 75   :     func_sad[make_tuple(4 , 2 , 2, NO_SIMD)] = Sad_AVX_C<4 , 2,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$03$01G@@YAIPBEH0H@Z ; Sad_AVX_C<4,2,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T6[ebp]
	push	eax
	lea	eax, DWORD PTR $T49[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T6[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 76   :     func_sad[make_tuple(4 , 1 , 2, NO_SIMD)] = Sad_AVX_C<4 , 1,uint16_t>;

	mov	eax, DWORD PTR $T49[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000002000000040000000200000000
	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$03$00G@@YAIPBEH0H@Z ; Sad_AVX_C<4,1,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T5[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T5[ebp], xmm0
	lea	eax, DWORD PTR $T48[ebp]
	push	eax
	lea	ecx, DWORD PTR _func_sad$[ebp]
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 77   :     func_sad[make_tuple(2 , 4 , 2, NO_SIMD)] = Sad_AVX_C<2 , 4,uint16_t>;

	mov	eax, DWORD PTR $T48[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000002000000020000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 77   :     func_sad[make_tuple(2 , 4 , 2, NO_SIMD)] = Sad_AVX_C<2 , 4,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$01$03G@@YAIPBEH0H@Z ; Sad_AVX_C<2,4,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T4[ebp]
	push	eax
	lea	eax, DWORD PTR $T47[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T4[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 78   :     func_sad[make_tuple(2 , 2 , 2, NO_SIMD)] = Sad_AVX_C<2 , 2,uint16_t>;

	mov	eax, DWORD PTR $T47[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
	vmovdqa	xmm0, XMMWORD PTR __xmm@00000002000000010000000200000000
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 78   :     func_sad[make_tuple(2 , 2 , 2, NO_SIMD)] = Sad_AVX_C<2 , 2,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$01$01G@@YAIPBEH0H@Z ; Sad_AVX_C<2,2,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T3[ebp]
	push	eax
	lea	eax, DWORD PTR $T46[ebp]
	push	eax
	vmovdqu	XMMWORD PTR $T3[ebp], xmm0
	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 79   :     func_sad[make_tuple(2 , 1 , 2, NO_SIMD)] = Sad_AVX_C<2 , 1,uint16_t>;

	mov	eax, DWORD PTR $T46[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	ecx, DWORD PTR _func_sad$[ebp]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 79   :     func_sad[make_tuple(2 , 1 , 2, NO_SIMD)] = Sad_AVX_C<2 , 1,uint16_t>;

	mov	DWORD PTR [eax+32], OFFSET ??$Sad_AVX_C@$01$00G@@YAIPBEH0H@Z ; Sad_AVX_C<2,1,unsigned short>
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\tuple

; 163  : 		: _Val(_STD forward<_Other>(_Arg))

	mov	eax, DWORD PTR _pixelsize$[ebp]
	mov	DWORD PTR $T2[ebp+4], eax
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	lea	eax, DWORD PTR $T2[ebp]
	push	eax
	lea	eax, DWORD PTR $T45[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\tuple

; 163  : 		: _Val(_STD forward<_Other>(_Arg))

	mov	DWORD PTR $T2[ebp], 0
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	push	eax
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\tuple

; 163  : 		: _Val(_STD forward<_Other>(_Arg))

	mov	DWORD PTR $T2[ebp+8], esi
	mov	DWORD PTR $T2[ebp+12], edi
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\map

; 244  : 		return (_Try_emplace(_STD move(_Keyval),

	call	??$_Try_emplace@V?$tuple@HHHW4arch_t@@@std@@$$V@?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE?AU?$pair@V?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@std@@_N@1@$$QAV?$tuple@HHHW4arch_t@@@1@@Z ; std::map<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> > >::_Try_emplace<std::tuple<int,int,int,enum arch_t> >
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 82   :     return result;

	mov	eax, DWORD PTR $T45[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\xtree

; 2168 : 		erase(begin(), end());

	lea	ecx, DWORD PTR _func_sad$[ebp]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 82   :     return result;

	mov	esi, DWORD PTR [eax+32]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\xtree

; 2168 : 		erase(begin(), end());

	mov	eax, DWORD PTR _func_sad$[ebp]
	push	eax
	push	DWORD PTR [eax]
	lea	eax, DWORD PTR $T88[ebp]
	push	eax
	call	?erase@?$_Tree@V?$_Tmap_traits@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@$0A@@std@@@std@@QAE?AV?$_Tree_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@2@V?$_Tree_const_iterator@V?$_Tree_val@U?$_Tree_simple_types@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@std@@@std@@@2@0@Z ; std::_Tree<std::_Tmap_traits<std::tuple<int,int,int,enum arch_t>,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int),std::less<std::tuple<int,int,int,enum arch_t> >,std::allocator<std::pair<std::tuple<int,int,int,enum arch_t> const ,unsigned int (__cdecl*)(unsigned char const *,int,unsigned char const *,int)> >,0> >::erase
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\xmemory0

; 720  : 		_Deallocate(_Ptr, _Count, sizeof (_Ty));

	mov	ecx, DWORD PTR _func_sad$[ebp]
	mov	edx, 1
	push	36					; 00000024H
	call	?_Deallocate@std@@YAXPAXII@Z		; std::_Deallocate
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 83   : }

	mov	ecx, DWORD PTR __$EHRec$[ebp]
; File c:\program files (x86)\microsoft visual studio 14.0\vc\include\xmemory0

; 720  : 		_Deallocate(_Ptr, _Count, sizeof (_Ty));

	add	esp, 4
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 82   :     return result;

	mov	eax, esi

; 83   : }

	mov	DWORD PTR fs:0, ecx
	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	ret	0
_TEXT	ENDS
;	COMDAT text$x
text$x	SEGMENT
__unwindfunclet$?get_sad_avx_C_function@@YAP6AIPBEH0H@ZHHHW4arch_t@@@Z$0:
	lea	ecx, DWORD PTR _func_sad$[ebp]
	jmp	??1?$map@V?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@ZU?$less@V?$tuple@HHHW4arch_t@@@std@@@2@V?$allocator@U?$pair@$$CBV?$tuple@HHHW4arch_t@@@std@@P6AIPBEH0H@Z@std@@@2@@std@@QAE@XZ
__ehhandler$?get_sad_avx_C_function@@YAP6AIPBEH0H@ZHHHW4arch_t@@@Z:
	mov	eax, OFFSET __ehfuncinfo$?get_sad_avx_C_function@@YAP6AIPBEH0H@ZHHHW4arch_t@@@Z
	jmp	___CxxFrameHandler3
text$x	ENDS
?get_sad_avx_C_function@@YAP6AIPBEH0H@ZHHHW4arch_t@@@Z ENDP ; get_sad_avx_C_function
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0CA@$0CA@E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0CA@$0CA@E@@YAIPBEH0H@Z PROC		; Sad_AVX_C<32,32,unsigned char>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 32					; 00000020H
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	xor	edx, edx
	mov	eax, esi
	sub	edi, esi
	npad	8
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [edi+eax]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovd	xmm0, DWORD PTR [edx+ecx+4]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax+4]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+8]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 32					; 00000020H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0CA@$0CA@E@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<32,32,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0CA@$0BA@E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0CA@$0BA@E@@YAIPBEH0H@Z PROC		; Sad_AVX_C<32,16,unsigned char>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 16					; 00000010H
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	xor	edx, edx
	mov	eax, esi
	sub	edi, esi
	npad	8
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [edi+eax]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovd	xmm0, DWORD PTR [edx+ecx+4]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax+4]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+8]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 32					; 00000020H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0CA@$0BA@E@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<32,16,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0CA@$07E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0CA@$07E@@YAIPBEH0H@Z PROC		; Sad_AVX_C<32,8,unsigned char>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 8
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	xor	edx, edx
	mov	eax, esi
	sub	edi, esi
	npad	8
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [edi+eax]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovd	xmm0, DWORD PTR [edx+ecx+4]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax+4]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+8]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 32					; 00000020H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0CA@$07E@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<32,8,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0BA@$0CA@E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0BA@$0CA@E@@YAIPBEH0H@Z PROC		; Sad_AVX_C<16,32,unsigned char>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 32					; 00000020H
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	xor	edx, edx
	mov	eax, esi
	sub	edi, esi
	npad	8
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [edi+eax]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovd	xmm0, DWORD PTR [edx+ecx+4]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax+4]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+8]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 16					; 00000010H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0BA@$0CA@E@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<16,32,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0BA@$0BA@E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0BA@$0BA@E@@YAIPBEH0H@Z PROC		; Sad_AVX_C<16,16,unsigned char>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 16					; 00000010H
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	xor	edx, edx
	mov	eax, esi
	sub	edi, esi
	npad	8
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [edi+eax]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovd	xmm0, DWORD PTR [edx+ecx+4]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax+4]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+8]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 16					; 00000010H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0BA@$0BA@E@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<16,16,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0BA@$07E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0BA@$07E@@YAIPBEH0H@Z PROC		; Sad_AVX_C<16,8,unsigned char>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 8
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	xor	edx, edx
	mov	eax, esi
	sub	edi, esi
	npad	8
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [edi+eax]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovd	xmm0, DWORD PTR [edx+ecx+4]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax+4]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+8]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 16					; 00000010H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0BA@$07E@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<16,8,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0BA@$03E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0BA@$03E@@YAIPBEH0H@Z PROC		; Sad_AVX_C<16,4,unsigned char>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 4
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	xor	edx, edx
	mov	eax, esi
	sub	edi, esi
	npad	8
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [edi+eax]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovd	xmm0, DWORD PTR [edx+ecx+4]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax+4]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+8]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 16					; 00000010H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0BA@$03E@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<16,4,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0BA@$01E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0BA@$01E@@YAIPBEH0H@Z PROC		; Sad_AVX_C<16,2,unsigned char>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 2
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	xor	edx, edx
	mov	eax, esi
	sub	edi, esi
	npad	8
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [edi+eax]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovd	xmm0, DWORD PTR [edx+ecx+4]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax+4]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+8]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 16					; 00000010H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0BA@$01E@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<16,2,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0BA@$00E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0BA@$00E@@YAIPBEH0H@Z PROC		; Sad_AVX_C<16,1,unsigned char>, COMDAT

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	xor	edx, edx
	mov	eax, DWORD PTR _pRef$[esp-4]
	push	esi
	vpxor	xmm2, xmm2, xmm2
	mov	esi, ecx
	vmovdqa	xmm3, xmm2
	sub	esi, eax
	npad	9
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [esi+eax]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovd	xmm0, DWORD PTR [ecx+edx+4]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax+4]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+8]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 16					; 00000010H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;
; 23   :     pRef += nRefPitch;
; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	vpaddd	xmm1, xmm1, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$0BA@$00E@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<16,1,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$07$0BA@E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$07$0BA@E@@YAIPBEH0H@Z PROC		; Sad_AVX_C<8,16,unsigned char>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	eax, DWORD PTR _pRef$[esp-4]
	mov	edx, 16					; 00000010H
	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[esp]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	mov	edi, DWORD PTR _nSrcPitch$[esp+4]
	vmovdqa	xmm3, xmm2
	npad	1
$LL4@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [ecx]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovd	xmm0, DWORD PTR [ecx+4]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax+4]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ecx, edi

; 23   :     pRef += nRefPitch;

	add	eax, esi
	vpaddd	xmm3, xmm0, xmm3
	sub	edx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	vpaddd	xmm1, xmm1, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	edi
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$07$0BA@E@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<8,16,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$07$07E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$07$07E@@YAIPBEH0H@Z PROC			; Sad_AVX_C<8,8,unsigned char>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	eax, DWORD PTR _pRef$[esp-4]
	mov	edx, 8
	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[esp]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	mov	edi, DWORD PTR _nSrcPitch$[esp+4]
	vmovdqa	xmm3, xmm2
	npad	1
$LL4@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [ecx]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovd	xmm0, DWORD PTR [ecx+4]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax+4]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ecx, edi

; 23   :     pRef += nRefPitch;

	add	eax, esi
	vpaddd	xmm3, xmm0, xmm3
	sub	edx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	vpaddd	xmm1, xmm1, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	edi
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$07$07E@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<8,8,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$07$03E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$07$03E@@YAIPBEH0H@Z PROC			; Sad_AVX_C<8,4,unsigned char>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	eax, DWORD PTR _pRef$[esp-4]
	mov	edx, 4
	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[esp]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	mov	edi, DWORD PTR _nSrcPitch$[esp+4]
	vmovdqa	xmm3, xmm2
	npad	1
$LL4@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [ecx]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovd	xmm0, DWORD PTR [ecx+4]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax+4]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ecx, edi

; 23   :     pRef += nRefPitch;

	add	eax, esi
	vpaddd	xmm3, xmm0, xmm3
	sub	edx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	vpaddd	xmm1, xmm1, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	edi
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$07$03E@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<8,4,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$07$01E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$07$01E@@YAIPBEH0H@Z PROC			; Sad_AVX_C<8,2,unsigned char>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	eax, DWORD PTR _pRef$[esp-4]
	mov	edx, 2
	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[esp]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	mov	edi, DWORD PTR _nSrcPitch$[esp+4]
	vmovdqa	xmm3, xmm2
	npad	1
$LL4@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [ecx]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovd	xmm0, DWORD PTR [ecx+4]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax+4]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ecx, edi

; 23   :     pRef += nRefPitch;

	add	eax, esi
	vpaddd	xmm3, xmm0, xmm3
	sub	edx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	vpaddd	xmm1, xmm1, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	edi
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$07$01E@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<8,2,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$07$00E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$07$00E@@YAIPBEH0H@Z PROC			; Sad_AVX_C<8,1,unsigned char>, COMDAT

; 15   : {

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	mov	eax, DWORD PTR _pRef$[esp-4]
	vmovd	xmm0, DWORD PTR [ecx]

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm2, xmm0
	vmovd	xmm0, DWORD PTR [ecx+4]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax+4]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm1, xmm0, xmm2
	vpsrldq	xmm0, xmm1, 8
	vpaddd	xmm1, xmm1, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	vmovd	eax, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 27   : }

	ret	0
??$Sad_AVX_C@$07$00E@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<8,1,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$03$07E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$03$07E@@YAIPBEH0H@Z PROC			; Sad_AVX_C<4,8,unsigned char>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	eax, DWORD PTR _pRef$[esp-4]
	mov	edx, 8
	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[esp]
	push	edi
	mov	edi, DWORD PTR _nSrcPitch$[esp+4]
	vpxor	xmm2, xmm2, xmm2
	npad	5
$LL4@Sad_AVX_C:

; 17   :   unsigned int sum = 0; // int is probably enough for 32x32
; 18   :   for ( int y = 0; y < nBlkHeight; y++ )
; 19   :   {
; 20   :     for ( int x = 0; x < nBlkWidth; x++ )
; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [ecx]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ecx, edi

; 23   :     pRef += nRefPitch;

	add	eax, esi
	vpaddd	xmm2, xmm0, xmm2
	sub	edx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpsrldq	xmm0, xmm2, 8
	vpaddd	xmm1, xmm2, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	edi
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$03$07E@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<4,8,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$03$03E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$03$03E@@YAIPBEH0H@Z PROC			; Sad_AVX_C<4,4,unsigned char>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	eax, DWORD PTR _pRef$[esp-4]
	mov	edx, 4
	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[esp]
	push	edi
	mov	edi, DWORD PTR _nSrcPitch$[esp+4]
	vpxor	xmm2, xmm2, xmm2
	npad	5
$LL4@Sad_AVX_C:

; 17   :   unsigned int sum = 0; // int is probably enough for 32x32
; 18   :   for ( int y = 0; y < nBlkHeight; y++ )
; 19   :   {
; 20   :     for ( int x = 0; x < nBlkWidth; x++ )
; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [ecx]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ecx, edi

; 23   :     pRef += nRefPitch;

	add	eax, esi
	vpaddd	xmm2, xmm0, xmm2
	sub	edx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpsrldq	xmm0, xmm2, 8
	vpaddd	xmm1, xmm2, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	edi
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$03$03E@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<4,4,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$03$01E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$03$01E@@YAIPBEH0H@Z PROC			; Sad_AVX_C<4,2,unsigned char>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	eax, DWORD PTR _pRef$[esp-4]
	mov	edx, 2
	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[esp]
	push	edi
	mov	edi, DWORD PTR _nSrcPitch$[esp+4]
	vpxor	xmm2, xmm2, xmm2
	npad	5
$LL4@Sad_AVX_C:

; 17   :   unsigned int sum = 0; // int is probably enough for 32x32
; 18   :   for ( int y = 0; y < nBlkHeight; y++ )
; 19   :   {
; 20   :     for ( int x = 0; x < nBlkWidth; x++ )
; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovd	xmm0, DWORD PTR [ecx]
	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ecx, edi

; 23   :     pRef += nRefPitch;

	add	eax, esi
	vpaddd	xmm2, xmm0, xmm2
	sub	edx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpsrldq	xmm0, xmm2, 8
	vpaddd	xmm1, xmm2, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	edi
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$03$01E@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<4,2,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$03$00E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$03$00E@@YAIPBEH0H@Z PROC			; Sad_AVX_C<4,1,unsigned char>, COMDAT

; 15   : {

	mov	eax, DWORD PTR _pSrc$[esp-4]
	vmovd	xmm0, DWORD PTR [eax]
	mov	eax, DWORD PTR _pRef$[esp-4]

; 16   :   _mm256_zeroupper();
; 17   :   unsigned int sum = 0; // int is probably enough for 32x32
; 18   :   for ( int y = 0; y < nBlkHeight; y++ )
; 19   :   {
; 20   :     for ( int x = 0; x < nBlkWidth; x++ )
; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxbd xmm1, xmm0
	vmovd	xmm0, DWORD PTR [eax]
	vpmovzxbd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm1, xmm0
	vpsrldq	xmm0, xmm1, 8
	vpaddd	xmm1, xmm1, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	vmovd	eax, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 27   : }

	ret	0
??$Sad_AVX_C@$03$00E@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<4,1,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$01$03E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_sum$1$ = -4						; size = 4
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$01$03E@@YAIPBEH0H@Z PROC			; Sad_AVX_C<2,4,unsigned char>, COMDAT

; 15   : {

	push	ecx
	push	ebx
	push	ebp
	push	esi

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	mov	esi, DWORD PTR _pSrc$[esp+12]
	push	edi
	mov	edi, DWORD PTR _pRef$[esp+16]

; 22   :     pSrc += nSrcPitch;

	mov	ebp, DWORD PTR _nSrcPitch$[esp+16]
	movzx	eax, BYTE PTR [esi+1]
	add	ebp, esi
	movzx	ecx, BYTE PTR [edi+1]
	sub	eax, ecx
	movzx	ecx, BYTE PTR [edi]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	mov	ebx, eax
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	eax, BYTE PTR [esi]
	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	ebx, edx
	sub	ebx, edx
	cdq
	xor	eax, edx
	sub	eax, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	add	ebx, eax
	movzx	eax, BYTE PTR [ebp+1]
	mov	DWORD PTR _sum$1$[esp+20], ebx

; 23   :     pRef += nRefPitch;

	mov	ebx, DWORD PTR _nRefPitch$[esp+16]
	add	ebx, edi
	movzx	ecx, BYTE PTR [ebx+1]
	sub	eax, ecx
	movzx	ecx, BYTE PTR [ebx]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	mov	edi, eax
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 23   :     pRef += nRefPitch;

	add	ebx, DWORD PTR _nRefPitch$[esp+16]
	movzx	eax, BYTE PTR [ebp]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	edi, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ebp, DWORD PTR _nSrcPitch$[esp+16]
	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	sub	edi, edx
	cdq
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	ecx, BYTE PTR [ebx+1]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	eax, edx
	sub	eax, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	add	eax, DWORD PTR _sum$1$[esp+20]
	add	edi, eax
	movzx	eax, BYTE PTR [ebp+1]
	sub	eax, ecx
	movzx	ecx, BYTE PTR [ebx]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	mov	esi, eax
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	eax, BYTE PTR [ebp]
	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	esi, edx
	sub	esi, edx
	cdq
	xor	eax, edx
	sub	eax, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	add	edi, eax
	mov	eax, DWORD PTR _nRefPitch$[esp+16]
	add	edi, esi
	movzx	ecx, BYTE PTR [ebx+eax+1]
	mov	eax, DWORD PTR _nSrcPitch$[esp+16]
	movzx	eax, BYTE PTR [eax+ebp+1]
	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	mov	esi, eax
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	mov	eax, DWORD PTR _nRefPitch$[esp+16]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	esi, edx
	sub	esi, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	ecx, BYTE PTR [ebx+eax]
	mov	eax, DWORD PTR _nSrcPitch$[esp+16]
	movzx	eax, BYTE PTR [eax+ebp]
	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	xor	eax, edx
	sub	eax, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	add	eax, edi
	pop	edi
	add	eax, esi
	pop	esi
	pop	ebp
	pop	ebx

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	pop	ecx
	ret	0
??$Sad_AVX_C@$01$03E@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<2,4,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$01$01E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$01$01E@@YAIPBEH0H@Z PROC			; Sad_AVX_C<2,2,unsigned char>, COMDAT

; 15   : {

	push	ebx

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	mov	ebx, DWORD PTR _pRef$[esp]
	push	ebp
	push	esi
	mov	esi, DWORD PTR _pSrc$[esp+8]
	movzx	ecx, BYTE PTR [ebx+1]
	push	edi

; 22   :     pSrc += nSrcPitch;

	mov	edi, DWORD PTR _nSrcPitch$[esp+12]
	movzx	eax, BYTE PTR [esi+1]
	add	edi, esi
	sub	eax, ecx
	movzx	ecx, BYTE PTR [ebx]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	mov	ebp, eax
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	eax, BYTE PTR [esi]

; 23   :     pRef += nRefPitch;

	mov	esi, DWORD PTR _nRefPitch$[esp+12]
	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	ebp, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 23   :     pRef += nRefPitch;

	add	esi, ebx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	sub	ebp, edx
	cdq
	xor	eax, edx
	sub	eax, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	ecx, BYTE PTR [esi+1]
	add	ebp, eax
	movzx	eax, BYTE PTR [edi+1]
	sub	eax, ecx
	movzx	ecx, BYTE PTR [esi]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	mov	ebx, eax
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	eax, BYTE PTR [edi]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	ebx, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	sub	ebx, edx
	cdq
	xor	eax, edx
	pop	edi
	sub	eax, edx
	pop	esi
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	add	eax, ebp
	pop	ebp
	add	eax, ebx
	pop	ebx

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	ret	0
??$Sad_AVX_C@$01$01E@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<2,2,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$01$00E@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$01$00E@@YAIPBEH0H@Z PROC			; Sad_AVX_C<2,1,unsigned char>, COMDAT

; 15   : {

	push	ebx
	push	esi

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	mov	esi, DWORD PTR _pRef$[esp+4]
	push	edi
	mov	edi, DWORD PTR _pSrc$[esp+8]
	movzx	ecx, BYTE PTR [esi+1]
	movzx	eax, BYTE PTR [edi+1]
	sub	eax, ecx
	movzx	ecx, BYTE PTR [esi]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	mov	ebx, eax
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	eax, BYTE PTR [edi]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	ebx, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	sub	ebx, edx
	cdq
	xor	eax, edx
	pop	edi
	sub	eax, edx
	pop	esi
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	add	eax, ebx
	pop	ebx

; 22   :     pSrc += nSrcPitch;
; 23   :     pRef += nRefPitch;
; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	ret	0
??$Sad_AVX_C@$01$00E@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<2,1,unsigned char>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0CA@$0CA@G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0CA@$0CA@G@@YAIPBEH0H@Z PROC		; Sad_AVX_C<32,32,unsigned short>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 32					; 00000020H
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	lea	eax, DWORD PTR [esi+8]
	xor	edx, edx
	sub	edi, esi
	npad	7
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [ecx+edx*2]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax-8]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovq	xmm0, QWORD PTR [edi+eax]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+16]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 32					; 00000020H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0CA@$0CA@G@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<32,32,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0CA@$0BA@G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0CA@$0BA@G@@YAIPBEH0H@Z PROC		; Sad_AVX_C<32,16,unsigned short>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 16					; 00000010H
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	lea	eax, DWORD PTR [esi+8]
	xor	edx, edx
	sub	edi, esi
	npad	7
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [ecx+edx*2]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax-8]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovq	xmm0, QWORD PTR [edi+eax]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+16]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 32					; 00000020H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0CA@$0BA@G@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<32,16,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0CA@$07G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0CA@$07G@@YAIPBEH0H@Z PROC		; Sad_AVX_C<32,8,unsigned short>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 8
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	lea	eax, DWORD PTR [esi+8]
	xor	edx, edx
	sub	edi, esi
	npad	7
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [ecx+edx*2]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax-8]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovq	xmm0, QWORD PTR [edi+eax]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+16]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 32					; 00000020H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0CA@$07G@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<32,8,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0BA@$0CA@G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0BA@$0CA@G@@YAIPBEH0H@Z PROC		; Sad_AVX_C<16,32,unsigned short>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 32					; 00000020H
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	lea	eax, DWORD PTR [esi+8]
	xor	edx, edx
	sub	edi, esi
	npad	7
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [ecx+edx*2]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax-8]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovq	xmm0, QWORD PTR [edi+eax]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+16]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 16					; 00000010H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0BA@$0CA@G@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<16,32,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0BA@$0BA@G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0BA@$0BA@G@@YAIPBEH0H@Z PROC		; Sad_AVX_C<16,16,unsigned short>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 16					; 00000010H
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	lea	eax, DWORD PTR [esi+8]
	xor	edx, edx
	sub	edi, esi
	npad	7
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [ecx+edx*2]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax-8]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovq	xmm0, QWORD PTR [edi+eax]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+16]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 16					; 00000010H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0BA@$0BA@G@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<16,16,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0BA@$07G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0BA@$07G@@YAIPBEH0H@Z PROC		; Sad_AVX_C<16,8,unsigned short>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 8
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	lea	eax, DWORD PTR [esi+8]
	xor	edx, edx
	sub	edi, esi
	npad	7
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [ecx+edx*2]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax-8]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovq	xmm0, QWORD PTR [edi+eax]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+16]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 16					; 00000010H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0BA@$07G@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<16,8,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0BA@$03G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0BA@$03G@@YAIPBEH0H@Z PROC		; Sad_AVX_C<16,4,unsigned short>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 4
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	lea	eax, DWORD PTR [esi+8]
	xor	edx, edx
	sub	edi, esi
	npad	7
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [ecx+edx*2]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax-8]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovq	xmm0, QWORD PTR [edi+eax]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+16]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 16					; 00000010H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0BA@$03G@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<16,4,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0BA@$01G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0BA@$01G@@YAIPBEH0H@Z PROC		; Sad_AVX_C<16,2,unsigned short>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	ebx
	push	ebp
	mov	ebp, DWORD PTR _nRefPitch$[esp+4]
	mov	ebx, 2
	push	esi
	mov	esi, DWORD PTR _pRef$[esp+8]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	vmovdqa	xmm3, xmm2
	npad	3
$LL4@Sad_AVX_C:

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	edi, ecx
	lea	eax, DWORD PTR [esi+8]
	xor	edx, edx
	sub	edi, esi
	npad	7
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [ecx+edx*2]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax-8]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovq	xmm0, QWORD PTR [edi+eax]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	eax, DWORD PTR [eax+16]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 16					; 00000010H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;

	add	ecx, DWORD PTR _nSrcPitch$[esp+12]

; 23   :     pRef += nRefPitch;

	add	esi, ebp
	sub	ebx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	pop	edi
	vpaddd	xmm1, xmm1, xmm0
	pop	esi
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	ebp
	vmovd	eax, xmm0
	pop	ebx
	ret	0
??$Sad_AVX_C@$0BA@$01G@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<16,2,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$0BA@$00G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$0BA@$00G@@YAIPBEH0H@Z PROC		; Sad_AVX_C<16,1,unsigned short>, COMDAT

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	mov	eax, DWORD PTR _pRef$[esp-4]
	xor	edx, edx
	push	esi
	push	edi
	mov	edi, DWORD PTR _pSrc$[esp+4]
	mov	esi, edi
	vpxor	xmm2, xmm2, xmm2
	vmovdqa	xmm3, xmm2
	lea	ecx, DWORD PTR [eax+8]
	sub	esi, eax
	npad	5
$LL7@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [edi+edx*2]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [ecx-8]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovq	xmm0, QWORD PTR [esi+ecx]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [ecx]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
	add	edx, 8
	lea	ecx, DWORD PTR [ecx+16]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm3, xmm0, xmm3
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 20   :     for ( int x = 0; x < nBlkWidth; x++ )

	cmp	edx, 16					; 00000010H
	jl	SHORT $LL7@Sad_AVX_C

; 22   :     pSrc += nSrcPitch;
; 23   :     pRef += nRefPitch;
; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	vpaddd	xmm1, xmm1, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	edi
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$0BA@$00G@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<16,1,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$07$0BA@G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$07$0BA@G@@YAIPBEH0H@Z PROC		; Sad_AVX_C<8,16,unsigned short>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	eax, DWORD PTR _pRef$[esp-4]
	mov	edx, 16					; 00000010H
	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[esp]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	mov	edi, DWORD PTR _nSrcPitch$[esp+4]
	vmovdqa	xmm3, xmm2
	npad	1
$LL4@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [ecx]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovq	xmm0, QWORD PTR [ecx+8]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax+8]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ecx, edi

; 23   :     pRef += nRefPitch;

	add	eax, esi
	vpaddd	xmm3, xmm0, xmm3
	sub	edx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	vpaddd	xmm1, xmm1, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	edi
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$07$0BA@G@@YAIPBEH0H@Z ENDP		; Sad_AVX_C<8,16,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$07$07G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$07$07G@@YAIPBEH0H@Z PROC			; Sad_AVX_C<8,8,unsigned short>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	eax, DWORD PTR _pRef$[esp-4]
	mov	edx, 8
	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[esp]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	mov	edi, DWORD PTR _nSrcPitch$[esp+4]
	vmovdqa	xmm3, xmm2
	npad	1
$LL4@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [ecx]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovq	xmm0, QWORD PTR [ecx+8]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax+8]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ecx, edi

; 23   :     pRef += nRefPitch;

	add	eax, esi
	vpaddd	xmm3, xmm0, xmm3
	sub	edx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	vpaddd	xmm1, xmm1, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	edi
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$07$07G@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<8,8,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$07$03G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$07$03G@@YAIPBEH0H@Z PROC			; Sad_AVX_C<8,4,unsigned short>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	eax, DWORD PTR _pRef$[esp-4]
	mov	edx, 4
	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[esp]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	mov	edi, DWORD PTR _nSrcPitch$[esp+4]
	vmovdqa	xmm3, xmm2
	npad	1
$LL4@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [ecx]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovq	xmm0, QWORD PTR [ecx+8]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax+8]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ecx, edi

; 23   :     pRef += nRefPitch;

	add	eax, esi
	vpaddd	xmm3, xmm0, xmm3
	sub	edx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	vpaddd	xmm1, xmm1, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	edi
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$07$03G@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<8,4,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$07$01G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$07$01G@@YAIPBEH0H@Z PROC			; Sad_AVX_C<8,2,unsigned short>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	eax, DWORD PTR _pRef$[esp-4]
	mov	edx, 2
	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[esp]
	vpxor	xmm2, xmm2, xmm2
	push	edi
	mov	edi, DWORD PTR _nSrcPitch$[esp+4]
	vmovdqa	xmm3, xmm2
	npad	1
$LL4@Sad_AVX_C:

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [ecx]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm2, xmm0, xmm2
	vmovq	xmm0, QWORD PTR [ecx+8]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax+8]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ecx, edi

; 23   :     pRef += nRefPitch;

	add	eax, esi
	vpaddd	xmm3, xmm0, xmm3
	sub	edx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpaddd	xmm1, xmm3, xmm2
	vpsrldq	xmm0, xmm1, 8
	vpaddd	xmm1, xmm1, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	edi
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$07$01G@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<8,2,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$07$00G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$07$00G@@YAIPBEH0H@Z PROC			; Sad_AVX_C<8,1,unsigned short>, COMDAT

; 15   : {

	mov	ecx, DWORD PTR _pSrc$[esp-4]
	mov	eax, DWORD PTR _pRef$[esp-4]
	vmovq	xmm0, QWORD PTR [ecx]

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm2, xmm0
	vmovq	xmm0, QWORD PTR [ecx+8]
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax+8]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
	vpaddd	xmm1, xmm0, xmm2
	vpsrldq	xmm0, xmm1, 8
	vpaddd	xmm1, xmm1, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	vmovd	eax, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 27   : }

	ret	0
??$Sad_AVX_C@$07$00G@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<8,1,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$03$07G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$03$07G@@YAIPBEH0H@Z PROC			; Sad_AVX_C<4,8,unsigned short>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	eax, DWORD PTR _pRef$[esp-4]
	mov	edx, 8
	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[esp]
	push	edi
	mov	edi, DWORD PTR _nSrcPitch$[esp+4]
	vpxor	xmm2, xmm2, xmm2
	npad	5
$LL4@Sad_AVX_C:

; 17   :   unsigned int sum = 0; // int is probably enough for 32x32
; 18   :   for ( int y = 0; y < nBlkHeight; y++ )
; 19   :   {
; 20   :     for ( int x = 0; x < nBlkWidth; x++ )
; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [ecx]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ecx, edi

; 23   :     pRef += nRefPitch;

	add	eax, esi
	vpaddd	xmm2, xmm0, xmm2
	sub	edx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpsrldq	xmm0, xmm2, 8
	vpaddd	xmm1, xmm2, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	edi
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$03$07G@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<4,8,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$03$03G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$03$03G@@YAIPBEH0H@Z PROC			; Sad_AVX_C<4,4,unsigned short>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	eax, DWORD PTR _pRef$[esp-4]
	mov	edx, 4
	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[esp]
	push	edi
	mov	edi, DWORD PTR _nSrcPitch$[esp+4]
	vpxor	xmm2, xmm2, xmm2
	npad	5
$LL4@Sad_AVX_C:

; 17   :   unsigned int sum = 0; // int is probably enough for 32x32
; 18   :   for ( int y = 0; y < nBlkHeight; y++ )
; 19   :   {
; 20   :     for ( int x = 0; x < nBlkWidth; x++ )
; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [ecx]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ecx, edi

; 23   :     pRef += nRefPitch;

	add	eax, esi
	vpaddd	xmm2, xmm0, xmm2
	sub	edx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpsrldq	xmm0, xmm2, 8
	vpaddd	xmm1, xmm2, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	edi
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$03$03G@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<4,4,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$03$01G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$03$01G@@YAIPBEH0H@Z PROC			; Sad_AVX_C<4,2,unsigned short>, COMDAT

; 16   :   _mm256_zeroupper();

	mov	eax, DWORD PTR _pRef$[esp-4]
	mov	edx, 2
	mov	ecx, DWORD PTR _pSrc$[esp-4]
	push	esi
	mov	esi, DWORD PTR _nRefPitch$[esp]
	push	edi
	mov	edi, DWORD PTR _nSrcPitch$[esp+4]
	vpxor	xmm2, xmm2, xmm2
	npad	5
$LL4@Sad_AVX_C:

; 17   :   unsigned int sum = 0; // int is probably enough for 32x32
; 18   :   for ( int y = 0; y < nBlkHeight; y++ )
; 19   :   {
; 20   :     for ( int x = 0; x < nBlkWidth; x++ )
; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vmovq	xmm0, QWORD PTR [ecx]
	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm0, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ecx, edi

; 23   :     pRef += nRefPitch;

	add	eax, esi
	vpaddd	xmm2, xmm0, xmm2
	sub	edx, 1
	jne	SHORT $LL4@Sad_AVX_C

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	vpsrldq	xmm0, xmm2, 8
	vpaddd	xmm1, xmm2, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	pop	edi
	vmovd	eax, xmm0
	pop	esi
	ret	0
??$Sad_AVX_C@$03$01G@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<4,2,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$03$00G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$03$00G@@YAIPBEH0H@Z PROC			; Sad_AVX_C<4,1,unsigned short>, COMDAT

; 15   : {

	mov	eax, DWORD PTR _pSrc$[esp-4]
	vmovq	xmm0, QWORD PTR [eax]
	mov	eax, DWORD PTR _pRef$[esp-4]

; 16   :   _mm256_zeroupper();
; 17   :   unsigned int sum = 0; // int is probably enough for 32x32
; 18   :   for ( int y = 0; y < nBlkHeight; y++ )
; 19   :   {
; 20   :     for ( int x = 0; x < nBlkWidth; x++ )
; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	vpmovzxwd xmm1, xmm0
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxwd xmm0, xmm0
	vpsubd	xmm0, xmm1, xmm0
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	vpabsd	xmm1, xmm0
	vpsrldq	xmm0, xmm1, 8
	vpaddd	xmm1, xmm1, xmm0
	vpsrldq	xmm0, xmm1, 4
	vpaddd	xmm0, xmm1, xmm0
	vmovd	eax, xmm0
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 27   : }

	ret	0
??$Sad_AVX_C@$03$00G@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<4,1,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$01$03G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_sum$1$ = -4						; size = 4
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$01$03G@@YAIPBEH0H@Z PROC			; Sad_AVX_C<2,4,unsigned short>, COMDAT

; 15   : {

	push	ecx
	push	ebx
	push	ebp
	push	esi

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	mov	esi, DWORD PTR _pSrc$[esp+12]
	push	edi
	mov	edi, DWORD PTR _pRef$[esp+16]

; 22   :     pSrc += nSrcPitch;

	mov	ebp, DWORD PTR _nSrcPitch$[esp+16]
	movzx	eax, WORD PTR [esi+2]
	add	ebp, esi
	movzx	ecx, WORD PTR [edi+2]
	sub	eax, ecx
	movzx	ecx, WORD PTR [edi]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	mov	ebx, eax
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	eax, WORD PTR [esi]
	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	ebx, edx
	sub	ebx, edx
	cdq
	xor	eax, edx
	sub	eax, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	add	ebx, eax
	movzx	eax, WORD PTR [ebp+2]
	mov	DWORD PTR _sum$1$[esp+20], ebx

; 23   :     pRef += nRefPitch;

	mov	ebx, DWORD PTR _nRefPitch$[esp+16]
	add	ebx, edi
	movzx	ecx, WORD PTR [ebx+2]
	sub	eax, ecx
	movzx	ecx, WORD PTR [ebx]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	mov	edi, eax
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 23   :     pRef += nRefPitch;

	add	ebx, DWORD PTR _nRefPitch$[esp+16]
	movzx	eax, WORD PTR [ebp]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	edi, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 22   :     pSrc += nSrcPitch;

	add	ebp, DWORD PTR _nSrcPitch$[esp+16]
	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	sub	edi, edx
	cdq
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	ecx, WORD PTR [ebx+2]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	eax, edx
	sub	eax, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	add	eax, DWORD PTR _sum$1$[esp+20]
	add	edi, eax
	movzx	eax, WORD PTR [ebp+2]
	sub	eax, ecx
	movzx	ecx, WORD PTR [ebx]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	mov	esi, eax
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	eax, WORD PTR [ebp]
	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	esi, edx
	sub	esi, edx
	cdq
	xor	eax, edx
	sub	eax, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	add	edi, eax
	mov	eax, DWORD PTR _nRefPitch$[esp+16]
	add	edi, esi
	movzx	ecx, WORD PTR [ebx+eax+2]
	mov	eax, DWORD PTR _nSrcPitch$[esp+16]
	movzx	eax, WORD PTR [eax+ebp+2]
	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	mov	esi, eax
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	mov	eax, DWORD PTR _nRefPitch$[esp+16]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	esi, edx
	sub	esi, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	ecx, WORD PTR [ebx+eax]
	mov	eax, DWORD PTR _nSrcPitch$[esp+16]
	movzx	eax, WORD PTR [eax+ebp]
	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	xor	eax, edx
	sub	eax, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	add	eax, edi
	pop	edi
	add	eax, esi
	pop	esi
	pop	ebp
	pop	ebx

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	pop	ecx
	ret	0
??$Sad_AVX_C@$01$03G@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<2,4,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$01$01G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$01$01G@@YAIPBEH0H@Z PROC			; Sad_AVX_C<2,2,unsigned short>, COMDAT

; 15   : {

	push	ebx

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	mov	ebx, DWORD PTR _pRef$[esp]
	push	ebp
	push	esi
	mov	esi, DWORD PTR _pSrc$[esp+8]
	movzx	ecx, WORD PTR [ebx+2]
	push	edi

; 22   :     pSrc += nSrcPitch;

	mov	edi, DWORD PTR _nSrcPitch$[esp+12]
	movzx	eax, WORD PTR [esi+2]
	add	edi, esi
	sub	eax, ecx
	movzx	ecx, WORD PTR [ebx]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	mov	ebp, eax
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	eax, WORD PTR [esi]

; 23   :     pRef += nRefPitch;

	mov	esi, DWORD PTR _nRefPitch$[esp+12]
	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	ebp, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 23   :     pRef += nRefPitch;

	add	esi, ebx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	sub	ebp, edx
	cdq
	xor	eax, edx
	sub	eax, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	ecx, WORD PTR [esi+2]
	add	ebp, eax
	movzx	eax, WORD PTR [edi+2]
	sub	eax, ecx
	movzx	ecx, WORD PTR [esi]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	mov	ebx, eax
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	eax, WORD PTR [edi]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	ebx, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	sub	ebx, edx
	cdq
	xor	eax, edx
	pop	edi
	sub	eax, edx
	pop	esi
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	add	eax, ebp
	pop	ebp
	add	eax, ebx
	pop	ebx

; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	ret	0
??$Sad_AVX_C@$01$01G@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<2,2,unsigned short>
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
; File c:\github\mvtools\sources\sadfunctions.cpp
; File c:\github\mvtools\sources\sadfunctions_avx.cpp
;	COMDAT ??$Sad_AVX_C@$01$00G@@YAIPBEH0H@Z
_TEXT	SEGMENT
_pSrc$ = 8						; size = 4
_nSrcPitch$ = 12					; size = 4
_pRef$ = 16						; size = 4
_nRefPitch$ = 20					; size = 4
??$Sad_AVX_C@$01$00G@@YAIPBEH0H@Z PROC			; Sad_AVX_C<2,1,unsigned short>, COMDAT

; 15   : {

	push	ebx
	push	esi

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	mov	esi, DWORD PTR _pRef$[esp+4]
	push	edi
	mov	edi, DWORD PTR _pSrc$[esp+8]
	movzx	ecx, WORD PTR [esi+2]
	movzx	eax, WORD PTR [edi+2]
	sub	eax, ecx
	movzx	ecx, WORD PTR [esi]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	cdq
	mov	ebx, eax
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	movzx	eax, WORD PTR [edi]
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	xor	ebx, edx
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	sub	eax, ecx
; File c:\github\mvtools\sources\sadfunctions.cpp

; 9    : inline unsigned int SADABS(int x) {	return ( x < 0 ) ? -x : x; }

	sub	ebx, edx
	cdq
	xor	eax, edx
	pop	edi
	sub	eax, edx
	pop	esi
; File c:\github\mvtools\sources\sadfunctions_avx.cpp

; 21   :       sum += SADABS(reinterpret_cast<const pixel_t *>(pSrc)[x] - reinterpret_cast<const pixel_t *>(pRef)[x]);

	add	eax, ebx
	pop	ebx

; 22   :     pSrc += nSrcPitch;
; 23   :     pRef += nRefPitch;
; 24   :   }
; 25   :   _mm256_zeroupper();
; 26   :   return sum;
; 27   : }

	ret	0
??$Sad_AVX_C@$01$00G@@YAIPBEH0H@Z ENDP			; Sad_AVX_C<2,1,unsigned short>
_TEXT	ENDS
END
